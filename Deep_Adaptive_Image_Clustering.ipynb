{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Adaptive_Image_Clustering.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2i0M1zYyWOU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9892ae4d-45b9-4fb2-ace4-72072832eddb"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import glob\n",
        "import numpy as np\n",
        "import PIL.Image as img\n",
        "import random\n",
        "from sklearn import metrics\n",
        "from sklearn.utils.linear_assignment_ import linear_assignment\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eie6X02oyZXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Option\n",
        "mode = 'Training'\n",
        "num_cluster = 10\n",
        "eps = 1e-10\n",
        "height = 28\n",
        "width = 28\n",
        "channel = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fKjuWFnyfBY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Datas and Labels as batch_size\n",
        "def get_batch(batch_size, img_data, imgt_labels):\n",
        "    batch_index = random.sample(range(len(imgt_labels)), batch_size)\n",
        "\n",
        "    batch_data = np.empty([batch_size, height, width, channel], dtype=np.float32)\n",
        "    batch_label = np.empty([batch_size], dtype=np.int32)\n",
        "\n",
        "    for n, i in enumerate(batch_index):\n",
        "        batch_data[n, ...] = img_data[i, ...]\n",
        "        batch_label[n] = imgt_labels[i]\n",
        "    return batch_data, batch_label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9ZlLwGPyj0J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get Datas and Labels as batch_size for Testing\n",
        "def get_batch_test(batch_size, img_data, i):\n",
        "    batch_data = np.copy(img_data[batch_size * i:batch_size * (i + 1), ...])\n",
        "    return batch_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j2R_MQnylvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clustering_acc(y_true, y_pred):\n",
        "    y_true = y_true.astype(np.int64)\n",
        "    assert y_pred.size == y_true.size\n",
        "    D = max(y_pred.max(), y_true.max()) + 1\n",
        "    w = np.zeros((D, D), dtype=np.int64)\n",
        "    for i in range(y_pred.size):\n",
        "        w[y_pred[i], y_true[i]] += 1\n",
        "    ind = linear_assignment(w.max() - w)\n",
        "\n",
        "    return sum([w[i, j] for i, j in ind]) * 1.0 / y_pred.size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iaLj0I4yncV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def NMI(y_true, y_pred):\n",
        "    return metrics.normalized_mutual_info_score(y_true, y_pred)\n",
        "\n",
        "def ARI(y_true, y_pred):\n",
        "    return metrics.adjusted_rand_score(y_true, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1ELT5WyypR-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.keras import backend as k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmdDJJhCyrCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e8490561-af5c-4b20-ddcc-4ab76700a6f5"
      },
      "source": [
        "tf.compat.v1.get_default_graph()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.framework.ops.Graph at 0x7ffab6e2d1d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4UGfpJyyso1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ConvNetwork(in_img, num_cluster, name='ConvNetwork', reuse=False):\n",
        "\twith tf.variable_scope(name, reuse=reuse):\n",
        "\t\t# conv1\n",
        "\t\tconv1 = tf.layers.conv2d(in_img, 64, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "\t\tconv1 = tf.layers.batch_normalization(conv1, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tconv1 = tf.nn.relu(conv1)\n",
        "\t\t# conv2\n",
        "\t\tconv2 = tf.layers.conv2d(conv1, 64, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "\t\tconv2 = tf.layers.batch_normalization(conv2, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tconv2 = tf.nn.relu(conv2)\n",
        "\t\t# conv3\n",
        "\t\tconv3 = tf.layers.conv2d(conv2, 64, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "\t\tconv3 = tf.layers.batch_normalization(conv3, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tconv3 = tf.nn.relu(conv3)\n",
        "\t\tconv3 = tf.layers.max_pooling2d(conv3, [2,2], [2,2])\n",
        "\t\tconv3 = tf.layers.batch_normalization(conv3, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\t# conv4\n",
        "\t\tconv4 = tf.layers.conv2d(conv3, 128, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "\t\tconv4 = tf.layers.batch_normalization(conv4, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tconv4 = tf.nn.relu(conv4)\n",
        "\t\t# conv5\n",
        "\t\tconv5 = tf.layers.conv2d(conv4, 128, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "\t\tconv5 = tf.layers.batch_normalization(conv5, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tconv5 = tf.nn.relu(conv5)\n",
        "\t\t# conv6\n",
        "\t\tconv6 = tf.layers.conv2d(conv5, 128, [3,3], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "\t\tconv6 = tf.layers.batch_normalization(conv6, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tconv6 = tf.nn.relu(conv6)\n",
        "\t\tconv6 = tf.layers.max_pooling2d(conv6, [2,2], [2,2])\n",
        "\t\tconv6 = tf.layers.batch_normalization(conv6, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\t# conv7\n",
        "\t\tconv7 = tf.layers.conv2d(conv6, 10, [1,1], [1,1], padding='valid', activation=None, kernel_initializer=tf.keras.initializers.he_normal())\n",
        "\t\tconv7 = tf.layers.batch_normalization(conv7, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tconv7 = tf.nn.relu(conv7)\n",
        "\t\tconv7 = tf.layers.average_pooling2d(conv7, [2,2], [2,2])\n",
        "\t\tconv7 = tf.layers.batch_normalization(conv7, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tconv7_flat = tf.layers.flatten(conv7)\n",
        "\n",
        "\t\t# dense8\n",
        "\t\tfc8 = tf.layers.dense(conv7_flat, 10, kernel_initializer=tf.initializers.identity())\n",
        "\t\tfc8 = tf.layers.batch_normalization(fc8, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tfc8 = tf.nn.relu(fc8)\n",
        "\t\t# dense9\n",
        "\t\tfc9 = tf.layers.dense(fc8, num_cluster, kernel_initializer=tf.initializers.identity())\n",
        "\t\tfc9 = tf.layers.batch_normalization(fc9, axis=-1, epsilon=1e-5, training=True, trainable=False)\n",
        "\t\tfc9 = tf.nn.relu(fc9)\n",
        "\n",
        "\t\tout = tf.nn.softmax(fc9)\n",
        "\n",
        "\treturn out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ-ZSvgZyuxF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0ed76938-2510-46f2-e27d-62e99fdd9e8c"
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k5xK4BOyws_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "499af090-7a3f-4b96-9628-7a20973a375a"
      },
      "source": [
        "image_pool_input = tf.placeholder(shape=[None, height, width, channel], dtype=tf.float32, name='image_pool_input')\n",
        "u_thres = tf.placeholder(shape=[], dtype=tf.float32, name='u_thres')\n",
        "l_thres = tf.placeholder(shape=[], dtype=tf.float32, name='l_thres')\n",
        "lr = tf.placeholder(shape=[], dtype=tf.float32, name='learning_rate')\n",
        "\n",
        "label_feat = ConvNetwork(image_pool_input, num_cluster, name='ConvNetwork', reuse=False)\n",
        "label_feat_norm = tf.nn.l2_normalize(label_feat, dim=1)\n",
        "sim_mat = tf.matmul(label_feat_norm, label_feat_norm, transpose_b=True)\n",
        "\n",
        "pos_loc = tf.greater(sim_mat, u_thres, name='greater')\n",
        "neg_loc = tf.less(sim_mat, l_thres, name='less')\n",
        "pos_loc_mask = tf.cast(pos_loc, dtype=tf.float32)\n",
        "neg_loc_mask = tf.cast(neg_loc, dtype=tf.float32)\n",
        "\n",
        "pred_label = tf.argmax(label_feat, axis=1)\n",
        "\n",
        "# Deep Adaptive Image Clustering Cost Function Optimize\n",
        "pos_entropy = tf.multiply(-tf.log(tf.clip_by_value(sim_mat, eps, 1.0)), pos_loc_mask)\n",
        "neg_entropy = tf.multiply(-tf.log(tf.clip_by_value(1-sim_mat, eps, 1.0)), neg_loc_mask)\n",
        "\n",
        "loss_sum = tf.reduce_mean(pos_entropy) + tf.reduce_mean(neg_entropy)\n",
        "train_op = tf.train.RMSPropOptimizer(lr).minimize(loss_sum)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-9-80e926f8058d>:4: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From <ipython-input-9-80e926f8058d>:5: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
            "WARNING:tensorflow:From <ipython-input-9-80e926f8058d>:15: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.MaxPooling2D instead.\n",
            "WARNING:tensorflow:From <ipython-input-9-80e926f8058d>:35: average_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.AveragePooling2D instead.\n",
            "WARNING:tensorflow:From <ipython-input-9-80e926f8058d>:37: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Flatten instead.\n",
            "WARNING:tensorflow:From <ipython-input-9-80e926f8058d>:40: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From <ipython-input-11-c6ce23d1053f>:7: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "dim is deprecated, use axis instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/rmsprop.py:123: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMlohC93yygS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9c170049-51c7-456e-f828-5f8188856e6c"
      },
      "source": [
        "import tensorflow as tf\n",
        "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvw9wUQky0q0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa0b27b6-1f50-4d87-92b3-e28b03c29448"
      },
      "source": [
        "mnist_train = np.reshape(train_images, (-1, 28, 28, 1))  # reshape into 1-channel image\n",
        "mnist_train_labels = np.asarray(train_labels, dtype=np.int32)\n",
        "mnist_test = np.reshape(test_images, (-1, 28, 28, 1))  # reshape into 1-channel image\n",
        "mnist_test_labels = np.asarray(test_labels, dtype=np.int32)\n",
        "image_data = np.concatenate([mnist_train, mnist_test], axis=0)\n",
        "image_label = np.concatenate([mnist_train_labels, mnist_test_labels], axis=0)\n",
        "print(len(image_data))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "70000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58ws_hHxzASM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compress Dimension\n",
        "mapping = {}\n",
        "mapped_label = 0\n",
        "for index,data in enumerate(image_label):\n",
        "  if(data in mapping):\n",
        "    image_label[index] = mapping[data]\n",
        "  else:\n",
        "    image_label[index] = mapping[data] = mapped_label+1\n",
        "    mapped_label += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHqzyruZzB_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nryTclCzDpw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-UyTymUzF2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3eedd022-ddcd-4d53-ec24-55c654eb4012"
      },
      "source": [
        "if mode == 'Training':\n",
        "    batch_size = 128\n",
        "    test_batch_size = 512\n",
        "    base_lr = 0.001\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        lamda = 0\n",
        "        epoch = 1\n",
        "        u = 0.95\n",
        "        l = 0.455\n",
        "\n",
        "        while u > l:\n",
        "            u = 0.95 - lamda\n",
        "            l = 0.455 + 0.1 * lamda\n",
        "            print(u, l)\n",
        "            for i in range(1, int(1001)):  # 1000 iterations is roughly 1 epoch\n",
        "                data_samples, _ = get_batch(batch_size, image_data, image_label)\n",
        "                feed_dict = {image_pool_input: data_samples, u_thres: u, l_thres: l, lr: base_lr}\n",
        "                train_loss, _ = sess.run([loss_sum, train_op], feed_dict=feed_dict)\n",
        "                if i % 20 == 0:\n",
        "                    print('training loss at iter %d is %f' % (i, train_loss))\n",
        "\n",
        "            lamda += 1.1 * 0.009\n",
        "            print(lamda)\n",
        "            # run testing every epoch\n",
        "            data_samples, data_labels = get_batch(test_batch_size, image_data, image_label)\n",
        "            feed_dict = {image_pool_input: data_samples}\n",
        "            pred_cluster = sess.run(pred_label, feed_dict=feed_dict)\n",
        "\n",
        "            acc = clustering_acc(data_labels, pred_cluster)\n",
        "            nmi = NMI(data_labels, pred_cluster)\n",
        "            ari = ARI(data_labels, pred_cluster)\n",
        "            print('testing NMI, ARI, ACC at epoch %d is %f, %f, %f.' % (epoch, nmi, ari, acc))\n",
        "\n",
        "            if epoch % 5 == 0:  # save model at every 5 epochs\n",
        "                model_name = 'DAC_ep_' + str(epoch) + '.ckpt'\n",
        "                save_path = saver.save(sess, 'DAC_models/' + model_name)\n",
        "                print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "            epoch += 1\n",
        "\n",
        "elif mode == 'Testing':\n",
        "    test_batch_size = 1000\n",
        "    with tf.Session() as sess:\n",
        "        saver.restore(sess, \"DAC_models/DAC_ep_45.ckpt\")\n",
        "        print('model restored!')\n",
        "        all_predictions = np.zeros([len(image_label)], dtype=np.float32)\n",
        "        for i in range(len(image_datsa) // test_batch_size):\n",
        "            data_samples = get_batch_test(test_batch_size, image_data, i)\n",
        "            feed_dict = {image_pool_input: data_samples}\n",
        "            pred_cluster = sess.run(pred_label, feed_dict=feed_dict)\n",
        "            all_predictions[i * test_batch_size:(i + 1) * test_batch_size] = pred_cluster\n",
        "\n",
        "        acc = clustering_acc(image_label.astype(int), all_predictions.astype(int))\n",
        "        nmi = NMI(image_label.astype(int), all_predictions.astype(int))\n",
        "        ari = ARI(image_label.astype(int), all_predictions.astype(int))\n",
        "        print('testing NMI, ARI, ACC are %f, %f, %f.' % (nmi, ari, acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.95 0.455\n",
            "training loss at iter 20 is 0.149347\n",
            "training loss at iter 40 is 0.170385\n",
            "training loss at iter 60 is 0.146097\n",
            "training loss at iter 80 is 0.178811\n",
            "training loss at iter 100 is 0.183752\n",
            "training loss at iter 120 is 0.175200\n",
            "training loss at iter 140 is 0.185121\n",
            "training loss at iter 160 is 0.177561\n",
            "training loss at iter 180 is 0.180105\n",
            "training loss at iter 200 is 0.179779\n",
            "training loss at iter 220 is 0.171554\n",
            "training loss at iter 240 is 0.162225\n",
            "training loss at iter 260 is 0.164176\n",
            "training loss at iter 280 is 0.174341\n",
            "training loss at iter 300 is 0.157272\n",
            "training loss at iter 320 is 0.151768\n",
            "training loss at iter 340 is 0.156998\n",
            "training loss at iter 360 is 0.142101\n",
            "training loss at iter 380 is 0.146148\n",
            "training loss at iter 400 is 0.153183\n",
            "training loss at iter 420 is 0.140798\n",
            "training loss at iter 440 is 0.157799\n",
            "training loss at iter 460 is 0.151539\n",
            "training loss at iter 480 is 0.143286\n",
            "training loss at iter 500 is 0.162643\n",
            "training loss at iter 520 is 0.149354\n",
            "training loss at iter 540 is 0.152002\n",
            "training loss at iter 560 is 0.142411\n",
            "training loss at iter 580 is 0.139940\n",
            "training loss at iter 600 is 0.136294\n",
            "training loss at iter 620 is 0.146293\n",
            "training loss at iter 640 is 0.143615\n",
            "training loss at iter 660 is 0.152362\n",
            "training loss at iter 680 is 0.143229\n",
            "training loss at iter 700 is 0.133349\n",
            "training loss at iter 720 is 0.143461\n",
            "training loss at iter 740 is 0.147107\n",
            "training loss at iter 760 is 0.139742\n",
            "training loss at iter 780 is 0.147143\n",
            "training loss at iter 800 is 0.136084\n",
            "training loss at iter 820 is 0.142167\n",
            "training loss at iter 840 is 0.132617\n",
            "training loss at iter 860 is 0.125861\n",
            "training loss at iter 880 is 0.153263\n",
            "training loss at iter 900 is 0.132620\n",
            "training loss at iter 920 is 0.150199\n",
            "training loss at iter 940 is 0.134422\n",
            "training loss at iter 960 is 0.138626\n",
            "training loss at iter 980 is 0.123846\n",
            "training loss at iter 1000 is 0.143415\n",
            "0.0099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "testing NMI, ARI, ACC at epoch 1 is 0.934073, 0.922542, 0.962891.\n",
            "0.9400999999999999 0.45599\n",
            "training loss at iter 20 is 0.138686\n",
            "training loss at iter 40 is 0.130629\n",
            "training loss at iter 60 is 0.137978\n",
            "training loss at iter 80 is 0.133647\n",
            "training loss at iter 100 is 0.140571\n",
            "training loss at iter 120 is 0.132992\n",
            "training loss at iter 140 is 0.138531\n",
            "training loss at iter 160 is 0.132507\n",
            "training loss at iter 180 is 0.137796\n",
            "training loss at iter 200 is 0.128005\n",
            "training loss at iter 220 is 0.136457\n",
            "training loss at iter 240 is 0.148888\n",
            "training loss at iter 260 is 0.143744\n",
            "training loss at iter 280 is 0.143197\n",
            "training loss at iter 300 is 0.132649\n",
            "training loss at iter 320 is 0.129489\n",
            "training loss at iter 340 is 0.135899\n",
            "training loss at iter 360 is 0.129418\n",
            "training loss at iter 380 is 0.137737\n",
            "training loss at iter 400 is 0.138591\n",
            "training loss at iter 420 is 0.129365\n",
            "training loss at iter 440 is 0.134367\n",
            "training loss at iter 460 is 0.133880\n",
            "training loss at iter 480 is 0.139443\n",
            "training loss at iter 500 is 0.132522\n",
            "training loss at iter 520 is 0.129070\n",
            "training loss at iter 540 is 0.136934\n",
            "training loss at iter 560 is 0.138294\n",
            "training loss at iter 580 is 0.135296\n",
            "training loss at iter 600 is 0.130327\n",
            "training loss at iter 620 is 0.136437\n",
            "training loss at iter 640 is 0.133686\n",
            "training loss at iter 660 is 0.147402\n",
            "training loss at iter 680 is 0.137269\n",
            "training loss at iter 700 is 0.136579\n",
            "training loss at iter 720 is 0.127624\n",
            "training loss at iter 740 is 0.129812\n",
            "training loss at iter 760 is 0.143236\n",
            "training loss at iter 780 is 0.144158\n",
            "training loss at iter 800 is 0.146256\n",
            "training loss at iter 820 is 0.135591\n",
            "training loss at iter 840 is 0.153688\n",
            "training loss at iter 860 is 0.139461\n",
            "training loss at iter 880 is 0.120177\n",
            "training loss at iter 900 is 0.148163\n",
            "training loss at iter 920 is 0.134520\n",
            "training loss at iter 940 is 0.136651\n",
            "training loss at iter 960 is 0.134784\n",
            "training loss at iter 980 is 0.130668\n",
            "training loss at iter 1000 is 0.130863\n",
            "0.0198\n",
            "testing NMI, ARI, ACC at epoch 2 is 0.941512, 0.929781, 0.968750.\n",
            "0.9301999999999999 0.45698\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.128675\n",
            "training loss at iter 40 is 0.138146\n",
            "training loss at iter 60 is 0.135212\n",
            "training loss at iter 80 is 0.133597\n",
            "training loss at iter 100 is 0.139735\n",
            "training loss at iter 120 is 0.136165\n",
            "training loss at iter 140 is 0.129509\n",
            "training loss at iter 160 is 0.133774\n",
            "training loss at iter 180 is 0.154692\n",
            "training loss at iter 200 is 0.124796\n",
            "training loss at iter 220 is 0.126286\n",
            "training loss at iter 240 is 0.135680\n",
            "training loss at iter 260 is 0.134470\n",
            "training loss at iter 280 is 0.136192\n",
            "training loss at iter 300 is 0.132227\n",
            "training loss at iter 320 is 0.125263\n",
            "training loss at iter 340 is 0.131121\n",
            "training loss at iter 360 is 0.161975\n",
            "training loss at iter 380 is 0.129970\n",
            "training loss at iter 400 is 0.137771\n",
            "training loss at iter 420 is 0.127105\n",
            "training loss at iter 440 is 0.145586\n",
            "training loss at iter 460 is 0.135014\n",
            "training loss at iter 480 is 0.131194\n",
            "training loss at iter 500 is 0.127329\n",
            "training loss at iter 520 is 0.131351\n",
            "training loss at iter 540 is 0.134658\n",
            "training loss at iter 560 is 0.126823\n",
            "training loss at iter 580 is 0.137628\n",
            "training loss at iter 600 is 0.123303\n",
            "training loss at iter 620 is 0.132128\n",
            "training loss at iter 640 is 0.134636\n",
            "training loss at iter 660 is 0.141744\n",
            "training loss at iter 680 is 0.126465\n",
            "training loss at iter 700 is 0.127640\n",
            "training loss at iter 720 is 0.123420\n",
            "training loss at iter 740 is 0.132714\n",
            "training loss at iter 760 is 0.126880\n",
            "training loss at iter 780 is 0.136699\n",
            "training loss at iter 800 is 0.137476\n",
            "training loss at iter 820 is 0.131427\n",
            "training loss at iter 840 is 0.132799\n",
            "training loss at iter 860 is 0.124522\n",
            "training loss at iter 880 is 0.129146\n",
            "training loss at iter 900 is 0.133932\n",
            "training loss at iter 920 is 0.144713\n",
            "training loss at iter 940 is 0.133547\n",
            "training loss at iter 960 is 0.142306\n",
            "training loss at iter 980 is 0.125879\n",
            "training loss at iter 1000 is 0.136011\n",
            "0.029700000000000004\n",
            "testing NMI, ARI, ACC at epoch 3 is 0.944899, 0.934618, 0.968750.\n",
            "0.9202999999999999 0.45797\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.132103\n",
            "training loss at iter 40 is 0.137584\n",
            "training loss at iter 60 is 0.127436\n",
            "training loss at iter 80 is 0.130922\n",
            "training loss at iter 100 is 0.130243\n",
            "training loss at iter 120 is 0.139164\n",
            "training loss at iter 140 is 0.126362\n",
            "training loss at iter 160 is 0.126222\n",
            "training loss at iter 180 is 0.125626\n",
            "training loss at iter 200 is 0.126723\n",
            "training loss at iter 220 is 0.129216\n",
            "training loss at iter 240 is 0.132542\n",
            "training loss at iter 260 is 0.122165\n",
            "training loss at iter 280 is 0.136705\n",
            "training loss at iter 300 is 0.128406\n",
            "training loss at iter 320 is 0.127426\n",
            "training loss at iter 340 is 0.140676\n",
            "training loss at iter 360 is 0.138218\n",
            "training loss at iter 380 is 0.136834\n",
            "training loss at iter 400 is 0.132061\n",
            "training loss at iter 420 is 0.145589\n",
            "training loss at iter 440 is 0.136262\n",
            "training loss at iter 460 is 0.124524\n",
            "training loss at iter 480 is 0.143575\n",
            "training loss at iter 500 is 0.134073\n",
            "training loss at iter 520 is 0.132753\n",
            "training loss at iter 540 is 0.121615\n",
            "training loss at iter 560 is 0.130874\n",
            "training loss at iter 580 is 0.126907\n",
            "training loss at iter 600 is 0.136404\n",
            "training loss at iter 620 is 0.136299\n",
            "training loss at iter 640 is 0.123184\n",
            "training loss at iter 660 is 0.133423\n",
            "training loss at iter 680 is 0.120290\n",
            "training loss at iter 700 is 0.131839\n",
            "training loss at iter 720 is 0.141913\n",
            "training loss at iter 740 is 0.136233\n",
            "training loss at iter 760 is 0.134506\n",
            "training loss at iter 780 is 0.123425\n",
            "training loss at iter 800 is 0.126511\n",
            "training loss at iter 820 is 0.124643\n",
            "training loss at iter 840 is 0.130347\n",
            "training loss at iter 860 is 0.134137\n",
            "training loss at iter 880 is 0.139229\n",
            "training loss at iter 900 is 0.126067\n",
            "training loss at iter 920 is 0.134750\n",
            "training loss at iter 940 is 0.131571\n",
            "training loss at iter 960 is 0.140056\n",
            "training loss at iter 980 is 0.125671\n",
            "training loss at iter 1000 is 0.141830\n",
            "0.0396\n",
            "testing NMI, ARI, ACC at epoch 4 is 0.929330, 0.915784, 0.962891.\n",
            "0.9104 0.45896000000000003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.126479\n",
            "training loss at iter 40 is 0.118806\n",
            "training loss at iter 60 is 0.122909\n",
            "training loss at iter 80 is 0.130580\n",
            "training loss at iter 100 is 0.131065\n",
            "training loss at iter 120 is 0.137201\n",
            "training loss at iter 140 is 0.126051\n",
            "training loss at iter 160 is 0.134796\n",
            "training loss at iter 180 is 0.133112\n",
            "training loss at iter 200 is 0.133104\n",
            "training loss at iter 220 is 0.131593\n",
            "training loss at iter 240 is 0.135058\n",
            "training loss at iter 260 is 0.126842\n",
            "training loss at iter 280 is 0.128256\n",
            "training loss at iter 300 is 0.123765\n",
            "training loss at iter 320 is 0.145869\n",
            "training loss at iter 340 is 0.122068\n",
            "training loss at iter 360 is 0.129137\n",
            "training loss at iter 380 is 0.144484\n",
            "training loss at iter 400 is 0.135602\n",
            "training loss at iter 420 is 0.130801\n",
            "training loss at iter 440 is 0.144104\n",
            "training loss at iter 460 is 0.138842\n",
            "training loss at iter 480 is 0.131049\n",
            "training loss at iter 500 is 0.136554\n",
            "training loss at iter 520 is 0.123968\n",
            "training loss at iter 540 is 0.123412\n",
            "training loss at iter 560 is 0.143846\n",
            "training loss at iter 580 is 0.124758\n",
            "training loss at iter 600 is 0.123343\n",
            "training loss at iter 620 is 0.121776\n",
            "training loss at iter 640 is 0.128843\n",
            "training loss at iter 660 is 0.128382\n",
            "training loss at iter 680 is 0.142374\n",
            "training loss at iter 700 is 0.128471\n",
            "training loss at iter 720 is 0.142386\n",
            "training loss at iter 740 is 0.127780\n",
            "training loss at iter 760 is 0.129016\n",
            "training loss at iter 780 is 0.131246\n",
            "training loss at iter 800 is 0.129210\n",
            "training loss at iter 820 is 0.138969\n",
            "training loss at iter 840 is 0.124761\n",
            "training loss at iter 860 is 0.129905\n",
            "training loss at iter 880 is 0.122612\n",
            "training loss at iter 900 is 0.128559\n",
            "training loss at iter 920 is 0.129770\n",
            "training loss at iter 940 is 0.124123\n",
            "training loss at iter 960 is 0.130439\n",
            "training loss at iter 980 is 0.132477\n",
            "training loss at iter 1000 is 0.129653\n",
            "0.0495\n",
            "testing NMI, ARI, ACC at epoch 5 is 0.946236, 0.927471, 0.966797.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model saved in file: DAC_models/DAC_ep_5.ckpt\n",
            "0.9005 0.45995\n",
            "training loss at iter 20 is 0.130365\n",
            "training loss at iter 40 is 0.121540\n",
            "training loss at iter 60 is 0.126319\n",
            "training loss at iter 80 is 0.131890\n",
            "training loss at iter 100 is 0.132039\n",
            "training loss at iter 120 is 0.133863\n",
            "training loss at iter 140 is 0.129829\n",
            "training loss at iter 160 is 0.132558\n",
            "training loss at iter 180 is 0.133879\n",
            "training loss at iter 200 is 0.131846\n",
            "training loss at iter 220 is 0.127363\n",
            "training loss at iter 240 is 0.127618\n",
            "training loss at iter 260 is 0.120233\n",
            "training loss at iter 280 is 0.124086\n",
            "training loss at iter 300 is 0.127008\n",
            "training loss at iter 320 is 0.133264\n",
            "training loss at iter 340 is 0.125215\n",
            "training loss at iter 360 is 0.125382\n",
            "training loss at iter 380 is 0.127317\n",
            "training loss at iter 400 is 0.133364\n",
            "training loss at iter 420 is 0.139372\n",
            "training loss at iter 440 is 0.138188\n",
            "training loss at iter 460 is 0.131074\n",
            "training loss at iter 480 is 0.137875\n",
            "training loss at iter 500 is 0.130418\n",
            "training loss at iter 520 is 0.130190\n",
            "training loss at iter 540 is 0.123957\n",
            "training loss at iter 560 is 0.133679\n",
            "training loss at iter 580 is 0.129835\n",
            "training loss at iter 600 is 0.143436\n",
            "training loss at iter 620 is 0.138280\n",
            "training loss at iter 640 is 0.138943\n",
            "training loss at iter 660 is 0.123410\n",
            "training loss at iter 680 is 0.134947\n",
            "training loss at iter 700 is 0.130933\n",
            "training loss at iter 720 is 0.125125\n",
            "training loss at iter 740 is 0.122242\n",
            "training loss at iter 760 is 0.130419\n",
            "training loss at iter 780 is 0.137826\n",
            "training loss at iter 800 is 0.138835\n",
            "training loss at iter 820 is 0.129882\n",
            "training loss at iter 840 is 0.130674\n",
            "training loss at iter 860 is 0.135305\n",
            "training loss at iter 880 is 0.134626\n",
            "training loss at iter 900 is 0.129085\n",
            "training loss at iter 920 is 0.131667\n",
            "training loss at iter 940 is 0.136154\n",
            "training loss at iter 960 is 0.126779\n",
            "training loss at iter 980 is 0.142121\n",
            "training loss at iter 1000 is 0.148913\n",
            "0.0594\n",
            "testing NMI, ARI, ACC at epoch 6 is 0.937475, 0.926865, 0.964844.\n",
            "0.8906 0.46094\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.138000\n",
            "training loss at iter 40 is 0.130275\n",
            "training loss at iter 60 is 0.128093\n",
            "training loss at iter 80 is 0.126625\n",
            "training loss at iter 100 is 0.138588\n",
            "training loss at iter 120 is 0.124782\n",
            "training loss at iter 140 is 0.136962\n",
            "training loss at iter 160 is 0.124754\n",
            "training loss at iter 180 is 0.132286\n",
            "training loss at iter 200 is 0.127871\n",
            "training loss at iter 220 is 0.141382\n",
            "training loss at iter 240 is 0.120555\n",
            "training loss at iter 260 is 0.124219\n",
            "training loss at iter 280 is 0.124882\n",
            "training loss at iter 300 is 0.122884\n",
            "training loss at iter 320 is 0.123245\n",
            "training loss at iter 340 is 0.144479\n",
            "training loss at iter 360 is 0.123476\n",
            "training loss at iter 380 is 0.131736\n",
            "training loss at iter 400 is 0.142274\n",
            "training loss at iter 420 is 0.138042\n",
            "training loss at iter 440 is 0.134362\n",
            "training loss at iter 460 is 0.125929\n",
            "training loss at iter 480 is 0.130217\n",
            "training loss at iter 500 is 0.125720\n",
            "training loss at iter 520 is 0.132305\n",
            "training loss at iter 540 is 0.125063\n",
            "training loss at iter 560 is 0.139017\n",
            "training loss at iter 580 is 0.130508\n",
            "training loss at iter 600 is 0.124192\n",
            "training loss at iter 620 is 0.122817\n",
            "training loss at iter 640 is 0.126618\n",
            "training loss at iter 660 is 0.139191\n",
            "training loss at iter 680 is 0.126607\n",
            "training loss at iter 700 is 0.124977\n",
            "training loss at iter 720 is 0.130636\n",
            "training loss at iter 740 is 0.119673\n",
            "training loss at iter 760 is 0.119696\n",
            "training loss at iter 780 is 0.137292\n",
            "training loss at iter 800 is 0.132431\n",
            "training loss at iter 820 is 0.137920\n",
            "training loss at iter 840 is 0.117690\n",
            "training loss at iter 860 is 0.123736\n",
            "training loss at iter 880 is 0.130451\n",
            "training loss at iter 900 is 0.138648\n",
            "training loss at iter 920 is 0.138491\n",
            "training loss at iter 940 is 0.132866\n",
            "training loss at iter 960 is 0.127269\n",
            "training loss at iter 980 is 0.133606\n",
            "training loss at iter 1000 is 0.137915\n",
            "0.0693\n",
            "testing NMI, ARI, ACC at epoch 7 is 0.930882, 0.909932, 0.958984.\n",
            "0.8806999999999999 0.46193\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.138268\n",
            "training loss at iter 40 is 0.145340\n",
            "training loss at iter 60 is 0.133365\n",
            "training loss at iter 80 is 0.136161\n",
            "training loss at iter 100 is 0.131206\n",
            "training loss at iter 120 is 0.128225\n",
            "training loss at iter 140 is 0.130612\n",
            "training loss at iter 160 is 0.122592\n",
            "training loss at iter 180 is 0.127103\n",
            "training loss at iter 200 is 0.123031\n",
            "training loss at iter 220 is 0.128396\n",
            "training loss at iter 240 is 0.137397\n",
            "training loss at iter 260 is 0.123944\n",
            "training loss at iter 280 is 0.126734\n",
            "training loss at iter 300 is 0.129135\n",
            "training loss at iter 320 is 0.142371\n",
            "training loss at iter 340 is 0.130875\n",
            "training loss at iter 360 is 0.123900\n",
            "training loss at iter 380 is 0.131883\n",
            "training loss at iter 400 is 0.122268\n",
            "training loss at iter 420 is 0.122948\n",
            "training loss at iter 440 is 0.134940\n",
            "training loss at iter 460 is 0.129136\n",
            "training loss at iter 480 is 0.130272\n",
            "training loss at iter 500 is 0.122429\n",
            "training loss at iter 520 is 0.128360\n",
            "training loss at iter 540 is 0.131448\n",
            "training loss at iter 560 is 0.119968\n",
            "training loss at iter 580 is 0.142923\n",
            "training loss at iter 600 is 0.130358\n",
            "training loss at iter 620 is 0.124011\n",
            "training loss at iter 640 is 0.131339\n",
            "training loss at iter 660 is 0.153977\n",
            "training loss at iter 680 is 0.125881\n",
            "training loss at iter 700 is 0.123346\n",
            "training loss at iter 720 is 0.150062\n",
            "training loss at iter 740 is 0.126033\n",
            "training loss at iter 760 is 0.136554\n",
            "training loss at iter 780 is 0.130650\n",
            "training loss at iter 800 is 0.128877\n",
            "training loss at iter 820 is 0.124299\n",
            "training loss at iter 840 is 0.136398\n",
            "training loss at iter 860 is 0.134964\n",
            "training loss at iter 880 is 0.129727\n",
            "training loss at iter 900 is 0.123543\n",
            "training loss at iter 920 is 0.130660\n",
            "training loss at iter 940 is 0.131329\n",
            "training loss at iter 960 is 0.137586\n",
            "training loss at iter 980 is 0.120662\n",
            "training loss at iter 1000 is 0.131827\n",
            "0.0792\n",
            "testing NMI, ARI, ACC at epoch 8 is 0.945929, 0.939424, 0.972656.\n",
            "0.8707999999999999 0.46292\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.130816\n",
            "training loss at iter 40 is 0.116777\n",
            "training loss at iter 60 is 0.122427\n",
            "training loss at iter 80 is 0.123479\n",
            "training loss at iter 100 is 0.116481\n",
            "training loss at iter 120 is 0.136878\n",
            "training loss at iter 140 is 0.129658\n",
            "training loss at iter 160 is 0.125684\n",
            "training loss at iter 180 is 0.140665\n",
            "training loss at iter 200 is 0.139662\n",
            "training loss at iter 220 is 0.134563\n",
            "training loss at iter 240 is 0.124682\n",
            "training loss at iter 260 is 0.118727\n",
            "training loss at iter 280 is 0.127520\n",
            "training loss at iter 300 is 0.138053\n",
            "training loss at iter 320 is 0.133603\n",
            "training loss at iter 340 is 0.130452\n",
            "training loss at iter 360 is 0.128048\n",
            "training loss at iter 380 is 0.128541\n",
            "training loss at iter 400 is 0.128726\n",
            "training loss at iter 420 is 0.147068\n",
            "training loss at iter 440 is 0.121642\n",
            "training loss at iter 460 is 0.124984\n",
            "training loss at iter 480 is 0.124937\n",
            "training loss at iter 500 is 0.132651\n",
            "training loss at iter 520 is 0.133847\n",
            "training loss at iter 540 is 0.125577\n",
            "training loss at iter 560 is 0.117871\n",
            "training loss at iter 580 is 0.121849\n",
            "training loss at iter 600 is 0.133972\n",
            "training loss at iter 620 is 0.123425\n",
            "training loss at iter 640 is 0.136929\n",
            "training loss at iter 660 is 0.123990\n",
            "training loss at iter 680 is 0.121620\n",
            "training loss at iter 700 is 0.126653\n",
            "training loss at iter 720 is 0.138999\n",
            "training loss at iter 740 is 0.118660\n",
            "training loss at iter 760 is 0.146192\n",
            "training loss at iter 780 is 0.122124\n",
            "training loss at iter 800 is 0.121987\n",
            "training loss at iter 820 is 0.125414\n",
            "training loss at iter 840 is 0.119858\n",
            "training loss at iter 860 is 0.125837\n",
            "training loss at iter 880 is 0.130424\n",
            "training loss at iter 900 is 0.130263\n",
            "training loss at iter 920 is 0.136650\n",
            "training loss at iter 940 is 0.141819\n",
            "training loss at iter 960 is 0.121242\n",
            "training loss at iter 980 is 0.131617\n",
            "training loss at iter 1000 is 0.133159\n",
            "0.08910000000000001\n",
            "testing NMI, ARI, ACC at epoch 9 is 0.957289, 0.946989, 0.976562.\n",
            "0.8609 0.46391000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.123930\n",
            "training loss at iter 40 is 0.125232\n",
            "training loss at iter 60 is 0.119550\n",
            "training loss at iter 80 is 0.135342\n",
            "training loss at iter 100 is 0.126110\n",
            "training loss at iter 120 is 0.119701\n",
            "training loss at iter 140 is 0.129489\n",
            "training loss at iter 160 is 0.123114\n",
            "training loss at iter 180 is 0.130830\n",
            "training loss at iter 200 is 0.119349\n",
            "training loss at iter 220 is 0.138540\n",
            "training loss at iter 240 is 0.119491\n",
            "training loss at iter 260 is 0.129762\n",
            "training loss at iter 280 is 0.121986\n",
            "training loss at iter 300 is 0.124642\n",
            "training loss at iter 320 is 0.130051\n",
            "training loss at iter 340 is 0.134164\n",
            "training loss at iter 360 is 0.130299\n",
            "training loss at iter 380 is 0.129963\n",
            "training loss at iter 400 is 0.142359\n",
            "training loss at iter 420 is 0.123675\n",
            "training loss at iter 440 is 0.124421\n",
            "training loss at iter 460 is 0.126349\n",
            "training loss at iter 480 is 0.136449\n",
            "training loss at iter 500 is 0.124040\n",
            "training loss at iter 520 is 0.129778\n",
            "training loss at iter 540 is 0.145713\n",
            "training loss at iter 560 is 0.133006\n",
            "training loss at iter 580 is 0.117922\n",
            "training loss at iter 600 is 0.123157\n",
            "training loss at iter 620 is 0.134018\n",
            "training loss at iter 640 is 0.129431\n",
            "training loss at iter 660 is 0.133874\n",
            "training loss at iter 680 is 0.137254\n",
            "training loss at iter 700 is 0.134358\n",
            "training loss at iter 720 is 0.133771\n",
            "training loss at iter 740 is 0.131639\n",
            "training loss at iter 760 is 0.120364\n",
            "training loss at iter 780 is 0.124357\n",
            "training loss at iter 800 is 0.119219\n",
            "training loss at iter 820 is 0.125250\n",
            "training loss at iter 840 is 0.124564\n",
            "training loss at iter 860 is 0.124285\n",
            "training loss at iter 880 is 0.129457\n",
            "training loss at iter 900 is 0.138497\n",
            "training loss at iter 920 is 0.121855\n",
            "training loss at iter 940 is 0.127742\n",
            "training loss at iter 960 is 0.123497\n",
            "training loss at iter 980 is 0.125813\n",
            "training loss at iter 1000 is 0.132321\n",
            "0.09900000000000002\n",
            "testing NMI, ARI, ACC at epoch 10 is 0.957113, 0.935056, 0.970703.\n",
            "Model saved in file: DAC_models/DAC_ep_10.ckpt\n",
            "0.851 0.46490000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.130654\n",
            "training loss at iter 40 is 0.129475\n",
            "training loss at iter 60 is 0.127941\n",
            "training loss at iter 80 is 0.128858\n",
            "training loss at iter 100 is 0.124480\n",
            "training loss at iter 120 is 0.136388\n",
            "training loss at iter 140 is 0.124441\n",
            "training loss at iter 160 is 0.136556\n",
            "training loss at iter 180 is 0.144488\n",
            "training loss at iter 200 is 0.129116\n",
            "training loss at iter 220 is 0.134017\n",
            "training loss at iter 240 is 0.118179\n",
            "training loss at iter 260 is 0.124313\n",
            "training loss at iter 280 is 0.130069\n",
            "training loss at iter 300 is 0.130926\n",
            "training loss at iter 320 is 0.145950\n",
            "training loss at iter 340 is 0.120217\n",
            "training loss at iter 360 is 0.122476\n",
            "training loss at iter 380 is 0.124266\n",
            "training loss at iter 400 is 0.121877\n",
            "training loss at iter 420 is 0.131788\n",
            "training loss at iter 440 is 0.121714\n",
            "training loss at iter 460 is 0.134229\n",
            "training loss at iter 480 is 0.121288\n",
            "training loss at iter 500 is 0.127531\n",
            "training loss at iter 520 is 0.122999\n",
            "training loss at iter 540 is 0.127421\n",
            "training loss at iter 560 is 0.127891\n",
            "training loss at iter 580 is 0.134349\n",
            "training loss at iter 600 is 0.120622\n",
            "training loss at iter 620 is 0.127083\n",
            "training loss at iter 640 is 0.127459\n",
            "training loss at iter 660 is 0.133626\n",
            "training loss at iter 680 is 0.126372\n",
            "training loss at iter 700 is 0.124433\n",
            "training loss at iter 720 is 0.132017\n",
            "training loss at iter 740 is 0.133305\n",
            "training loss at iter 760 is 0.124421\n",
            "training loss at iter 780 is 0.119849\n",
            "training loss at iter 800 is 0.121957\n",
            "training loss at iter 820 is 0.124731\n",
            "training loss at iter 840 is 0.126990\n",
            "training loss at iter 860 is 0.133335\n",
            "training loss at iter 880 is 0.122454\n",
            "training loss at iter 900 is 0.146688\n",
            "training loss at iter 920 is 0.131006\n",
            "training loss at iter 940 is 0.134796\n",
            "training loss at iter 960 is 0.129471\n",
            "training loss at iter 980 is 0.127387\n",
            "training loss at iter 1000 is 0.128969\n",
            "0.10890000000000002\n",
            "testing NMI, ARI, ACC at epoch 11 is 0.962348, 0.947580, 0.978516.\n",
            "0.8411 0.46589\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.129712\n",
            "training loss at iter 40 is 0.136329\n",
            "training loss at iter 60 is 0.134464\n",
            "training loss at iter 80 is 0.133661\n",
            "training loss at iter 100 is 0.129044\n",
            "training loss at iter 120 is 0.122358\n",
            "training loss at iter 140 is 0.122310\n",
            "training loss at iter 160 is 0.134485\n",
            "training loss at iter 180 is 0.129605\n",
            "training loss at iter 200 is 0.131685\n",
            "training loss at iter 220 is 0.128758\n",
            "training loss at iter 240 is 0.135065\n",
            "training loss at iter 260 is 0.140519\n",
            "training loss at iter 280 is 0.130680\n",
            "training loss at iter 300 is 0.123329\n",
            "training loss at iter 320 is 0.135941\n",
            "training loss at iter 340 is 0.129153\n",
            "training loss at iter 360 is 0.135891\n",
            "training loss at iter 380 is 0.125979\n",
            "training loss at iter 400 is 0.131065\n",
            "training loss at iter 420 is 0.128979\n",
            "training loss at iter 440 is 0.134200\n",
            "training loss at iter 460 is 0.123696\n",
            "training loss at iter 480 is 0.128418\n",
            "training loss at iter 500 is 0.132863\n",
            "training loss at iter 520 is 0.129039\n",
            "training loss at iter 540 is 0.135978\n",
            "training loss at iter 560 is 0.120649\n",
            "training loss at iter 580 is 0.125175\n",
            "training loss at iter 600 is 0.124208\n",
            "training loss at iter 620 is 0.135674\n",
            "training loss at iter 640 is 0.131317\n",
            "training loss at iter 660 is 0.134024\n",
            "training loss at iter 680 is 0.141041\n",
            "training loss at iter 700 is 0.117939\n",
            "training loss at iter 720 is 0.138070\n",
            "training loss at iter 740 is 0.127104\n",
            "training loss at iter 760 is 0.122570\n",
            "training loss at iter 780 is 0.122657\n",
            "training loss at iter 800 is 0.119088\n",
            "training loss at iter 820 is 0.117238\n",
            "training loss at iter 840 is 0.144237\n",
            "training loss at iter 860 is 0.131910\n",
            "training loss at iter 880 is 0.124811\n",
            "training loss at iter 900 is 0.125906\n",
            "training loss at iter 920 is 0.128751\n",
            "training loss at iter 940 is 0.136950\n",
            "training loss at iter 960 is 0.135812\n",
            "training loss at iter 980 is 0.121861\n",
            "training loss at iter 1000 is 0.138296\n",
            "0.11880000000000003\n",
            "testing NMI, ARI, ACC at epoch 12 is 0.965921, 0.962849, 0.982422.\n",
            "0.8311999999999999 0.46688\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.119364\n",
            "training loss at iter 40 is 0.127907\n",
            "training loss at iter 60 is 0.127231\n",
            "training loss at iter 80 is 0.144315\n",
            "training loss at iter 100 is 0.126136\n",
            "training loss at iter 120 is 0.127911\n",
            "training loss at iter 140 is 0.118507\n",
            "training loss at iter 160 is 0.137702\n",
            "training loss at iter 180 is 0.119912\n",
            "training loss at iter 200 is 0.129957\n",
            "training loss at iter 220 is 0.126248\n",
            "training loss at iter 240 is 0.123113\n",
            "training loss at iter 260 is 0.121928\n",
            "training loss at iter 280 is 0.120557\n",
            "training loss at iter 300 is 0.126822\n",
            "training loss at iter 320 is 0.140061\n",
            "training loss at iter 340 is 0.124237\n",
            "training loss at iter 360 is 0.120730\n",
            "training loss at iter 380 is 0.126650\n",
            "training loss at iter 400 is 0.129497\n",
            "training loss at iter 420 is 0.125933\n",
            "training loss at iter 440 is 0.120162\n",
            "training loss at iter 460 is 0.118871\n",
            "training loss at iter 480 is 0.128834\n",
            "training loss at iter 500 is 0.130732\n",
            "training loss at iter 520 is 0.124360\n",
            "training loss at iter 540 is 0.136985\n",
            "training loss at iter 560 is 0.121101\n",
            "training loss at iter 580 is 0.122898\n",
            "training loss at iter 600 is 0.122405\n",
            "training loss at iter 620 is 0.119119\n",
            "training loss at iter 640 is 0.117975\n",
            "training loss at iter 660 is 0.130907\n",
            "training loss at iter 680 is 0.120959\n",
            "training loss at iter 700 is 0.139857\n",
            "training loss at iter 720 is 0.138374\n",
            "training loss at iter 740 is 0.138550\n",
            "training loss at iter 760 is 0.132123\n",
            "training loss at iter 780 is 0.143469\n",
            "training loss at iter 800 is 0.126585\n",
            "training loss at iter 820 is 0.128020\n",
            "training loss at iter 840 is 0.124859\n",
            "training loss at iter 860 is 0.122971\n",
            "training loss at iter 880 is 0.139500\n",
            "training loss at iter 900 is 0.123876\n",
            "training loss at iter 920 is 0.131302\n",
            "training loss at iter 940 is 0.130550\n",
            "training loss at iter 960 is 0.127648\n",
            "training loss at iter 980 is 0.129031\n",
            "training loss at iter 1000 is 0.116669\n",
            "0.12870000000000004\n",
            "testing NMI, ARI, ACC at epoch 13 is 0.949963, 0.933996, 0.968750.\n",
            "0.8212999999999999 0.46787\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.128555\n",
            "training loss at iter 40 is 0.128135\n",
            "training loss at iter 60 is 0.127762\n",
            "training loss at iter 80 is 0.140331\n",
            "training loss at iter 100 is 0.135449\n",
            "training loss at iter 120 is 0.122696\n",
            "training loss at iter 140 is 0.130809\n",
            "training loss at iter 160 is 0.132521\n",
            "training loss at iter 180 is 0.126706\n",
            "training loss at iter 200 is 0.125890\n",
            "training loss at iter 220 is 0.125999\n",
            "training loss at iter 240 is 0.123056\n",
            "training loss at iter 260 is 0.141497\n",
            "training loss at iter 280 is 0.122384\n",
            "training loss at iter 300 is 0.119925\n",
            "training loss at iter 320 is 0.127952\n",
            "training loss at iter 340 is 0.135182\n",
            "training loss at iter 360 is 0.129022\n",
            "training loss at iter 380 is 0.133653\n",
            "training loss at iter 400 is 0.132134\n",
            "training loss at iter 420 is 0.125309\n",
            "training loss at iter 440 is 0.133237\n",
            "training loss at iter 460 is 0.128126\n",
            "training loss at iter 480 is 0.121168\n",
            "training loss at iter 500 is 0.133788\n",
            "training loss at iter 520 is 0.130272\n",
            "training loss at iter 540 is 0.133363\n",
            "training loss at iter 560 is 0.122370\n",
            "training loss at iter 580 is 0.122572\n",
            "training loss at iter 600 is 0.136685\n",
            "training loss at iter 620 is 0.125255\n",
            "training loss at iter 640 is 0.133381\n",
            "training loss at iter 660 is 0.138043\n",
            "training loss at iter 680 is 0.126306\n",
            "training loss at iter 700 is 0.140003\n",
            "training loss at iter 720 is 0.121754\n",
            "training loss at iter 740 is 0.127296\n",
            "training loss at iter 760 is 0.130126\n",
            "training loss at iter 780 is 0.126137\n",
            "training loss at iter 800 is 0.129651\n",
            "training loss at iter 820 is 0.137924\n",
            "training loss at iter 840 is 0.123155\n",
            "training loss at iter 860 is 0.132070\n",
            "training loss at iter 880 is 0.123187\n",
            "training loss at iter 900 is 0.125458\n",
            "training loss at iter 920 is 0.121820\n",
            "training loss at iter 940 is 0.122596\n",
            "training loss at iter 960 is 0.124537\n",
            "training loss at iter 980 is 0.125350\n",
            "training loss at iter 1000 is 0.123283\n",
            "0.13860000000000003\n",
            "testing NMI, ARI, ACC at epoch 14 is 0.960950, 0.954618, 0.978516.\n",
            "0.8113999999999999 0.46886\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.123091\n",
            "training loss at iter 40 is 0.137464\n",
            "training loss at iter 60 is 0.122923\n",
            "training loss at iter 80 is 0.125199\n",
            "training loss at iter 100 is 0.129394\n",
            "training loss at iter 120 is 0.120881\n",
            "training loss at iter 140 is 0.125305\n",
            "training loss at iter 160 is 0.125167\n",
            "training loss at iter 180 is 0.124994\n",
            "training loss at iter 200 is 0.122946\n",
            "training loss at iter 220 is 0.139144\n",
            "training loss at iter 240 is 0.143768\n",
            "training loss at iter 260 is 0.127144\n",
            "training loss at iter 280 is 0.135055\n",
            "training loss at iter 300 is 0.130720\n",
            "training loss at iter 320 is 0.138442\n",
            "training loss at iter 340 is 0.124689\n",
            "training loss at iter 360 is 0.120328\n",
            "training loss at iter 380 is 0.132575\n",
            "training loss at iter 400 is 0.138379\n",
            "training loss at iter 420 is 0.126614\n",
            "training loss at iter 440 is 0.114886\n",
            "training loss at iter 460 is 0.121491\n",
            "training loss at iter 480 is 0.142049\n",
            "training loss at iter 500 is 0.143365\n",
            "training loss at iter 520 is 0.128767\n",
            "training loss at iter 540 is 0.118250\n",
            "training loss at iter 560 is 0.130079\n",
            "training loss at iter 580 is 0.125286\n",
            "training loss at iter 600 is 0.126317\n",
            "training loss at iter 620 is 0.124709\n",
            "training loss at iter 640 is 0.124594\n",
            "training loss at iter 660 is 0.124392\n",
            "training loss at iter 680 is 0.124886\n",
            "training loss at iter 700 is 0.131131\n",
            "training loss at iter 720 is 0.134965\n",
            "training loss at iter 740 is 0.128077\n",
            "training loss at iter 760 is 0.133657\n",
            "training loss at iter 780 is 0.123129\n",
            "training loss at iter 800 is 0.140990\n",
            "training loss at iter 820 is 0.129175\n",
            "training loss at iter 840 is 0.140546\n",
            "training loss at iter 860 is 0.131139\n",
            "training loss at iter 880 is 0.133315\n",
            "training loss at iter 900 is 0.124110\n",
            "training loss at iter 920 is 0.130813\n",
            "training loss at iter 940 is 0.129817\n",
            "training loss at iter 960 is 0.135074\n",
            "training loss at iter 980 is 0.129298\n",
            "training loss at iter 1000 is 0.120218\n",
            "0.14850000000000002\n",
            "testing NMI, ARI, ACC at epoch 15 is 0.954292, 0.949186, 0.974609.\n",
            "Model saved in file: DAC_models/DAC_ep_15.ckpt\n",
            "0.8014999999999999 0.46985\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.117489\n",
            "training loss at iter 40 is 0.129038\n",
            "training loss at iter 60 is 0.131607\n",
            "training loss at iter 80 is 0.126673\n",
            "training loss at iter 100 is 0.125091\n",
            "training loss at iter 120 is 0.119097\n",
            "training loss at iter 140 is 0.133425\n",
            "training loss at iter 160 is 0.141340\n",
            "training loss at iter 180 is 0.137259\n",
            "training loss at iter 200 is 0.135277\n",
            "training loss at iter 220 is 0.121438\n",
            "training loss at iter 240 is 0.123989\n",
            "training loss at iter 260 is 0.133556\n",
            "training loss at iter 280 is 0.133680\n",
            "training loss at iter 300 is 0.123720\n",
            "training loss at iter 320 is 0.122777\n",
            "training loss at iter 340 is 0.139086\n",
            "training loss at iter 360 is 0.120826\n",
            "training loss at iter 380 is 0.115098\n",
            "training loss at iter 400 is 0.145535\n",
            "training loss at iter 420 is 0.135764\n",
            "training loss at iter 440 is 0.124589\n",
            "training loss at iter 460 is 0.127561\n",
            "training loss at iter 480 is 0.125415\n",
            "training loss at iter 500 is 0.129210\n",
            "training loss at iter 520 is 0.127381\n",
            "training loss at iter 540 is 0.129330\n",
            "training loss at iter 560 is 0.133065\n",
            "training loss at iter 580 is 0.131068\n",
            "training loss at iter 600 is 0.137081\n",
            "training loss at iter 620 is 0.130247\n",
            "training loss at iter 640 is 0.145064\n",
            "training loss at iter 660 is 0.123677\n",
            "training loss at iter 680 is 0.124243\n",
            "training loss at iter 700 is 0.122093\n",
            "training loss at iter 720 is 0.131243\n",
            "training loss at iter 740 is 0.118148\n",
            "training loss at iter 760 is 0.140797\n",
            "training loss at iter 780 is 0.138640\n",
            "training loss at iter 800 is 0.121446\n",
            "training loss at iter 820 is 0.129997\n",
            "training loss at iter 840 is 0.129095\n",
            "training loss at iter 860 is 0.126095\n",
            "training loss at iter 880 is 0.125481\n",
            "training loss at iter 900 is 0.133551\n",
            "training loss at iter 920 is 0.124493\n",
            "training loss at iter 940 is 0.123879\n",
            "training loss at iter 960 is 0.121133\n",
            "training loss at iter 980 is 0.127965\n",
            "training loss at iter 1000 is 0.130955\n",
            "0.1584\n",
            "testing NMI, ARI, ACC at epoch 16 is 0.937647, 0.923725, 0.964844.\n",
            "0.7916 0.47084000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.119157\n",
            "training loss at iter 40 is 0.117377\n",
            "training loss at iter 60 is 0.122768\n",
            "training loss at iter 80 is 0.119748\n",
            "training loss at iter 100 is 0.124867\n",
            "training loss at iter 120 is 0.134591\n",
            "training loss at iter 140 is 0.125435\n",
            "training loss at iter 160 is 0.134130\n",
            "training loss at iter 180 is 0.121530\n",
            "training loss at iter 200 is 0.126162\n",
            "training loss at iter 220 is 0.118490\n",
            "training loss at iter 240 is 0.135178\n",
            "training loss at iter 260 is 0.136771\n",
            "training loss at iter 280 is 0.124673\n",
            "training loss at iter 300 is 0.118391\n",
            "training loss at iter 320 is 0.120795\n",
            "training loss at iter 340 is 0.128638\n",
            "training loss at iter 360 is 0.130788\n",
            "training loss at iter 380 is 0.125903\n",
            "training loss at iter 400 is 0.117994\n",
            "training loss at iter 420 is 0.132846\n",
            "training loss at iter 440 is 0.120370\n",
            "training loss at iter 460 is 0.130907\n",
            "training loss at iter 480 is 0.143615\n",
            "training loss at iter 500 is 0.124118\n",
            "training loss at iter 520 is 0.130774\n",
            "training loss at iter 540 is 0.117705\n",
            "training loss at iter 560 is 0.136790\n",
            "training loss at iter 580 is 0.131726\n",
            "training loss at iter 600 is 0.120727\n",
            "training loss at iter 620 is 0.157562\n",
            "training loss at iter 640 is 0.138247\n",
            "training loss at iter 660 is 0.136071\n",
            "training loss at iter 680 is 0.121346\n",
            "training loss at iter 700 is 0.129171\n",
            "training loss at iter 720 is 0.124372\n",
            "training loss at iter 740 is 0.118855\n",
            "training loss at iter 760 is 0.126515\n",
            "training loss at iter 780 is 0.126083\n",
            "training loss at iter 800 is 0.130463\n",
            "training loss at iter 820 is 0.130980\n",
            "training loss at iter 840 is 0.133234\n",
            "training loss at iter 860 is 0.128495\n",
            "training loss at iter 880 is 0.127506\n",
            "training loss at iter 900 is 0.121900\n",
            "training loss at iter 920 is 0.127942\n",
            "training loss at iter 940 is 0.127531\n",
            "training loss at iter 960 is 0.127405\n",
            "training loss at iter 980 is 0.127658\n",
            "training loss at iter 1000 is 0.138687\n",
            "0.1683\n",
            "testing NMI, ARI, ACC at epoch 17 is 0.949692, 0.939731, 0.972656.\n",
            "0.7817 0.47183\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.144066\n",
            "training loss at iter 40 is 0.127121\n",
            "training loss at iter 60 is 0.130903\n",
            "training loss at iter 80 is 0.126033\n",
            "training loss at iter 100 is 0.143729\n",
            "training loss at iter 120 is 0.126891\n",
            "training loss at iter 140 is 0.122964\n",
            "training loss at iter 160 is 0.127864\n",
            "training loss at iter 180 is 0.132006\n",
            "training loss at iter 200 is 0.122412\n",
            "training loss at iter 220 is 0.116431\n",
            "training loss at iter 240 is 0.131416\n",
            "training loss at iter 260 is 0.131125\n",
            "training loss at iter 280 is 0.123982\n",
            "training loss at iter 300 is 0.137662\n",
            "training loss at iter 320 is 0.123038\n",
            "training loss at iter 340 is 0.118928\n",
            "training loss at iter 360 is 0.132774\n",
            "training loss at iter 380 is 0.121902\n",
            "training loss at iter 400 is 0.131524\n",
            "training loss at iter 420 is 0.134241\n",
            "training loss at iter 440 is 0.125134\n",
            "training loss at iter 460 is 0.135687\n",
            "training loss at iter 480 is 0.143240\n",
            "training loss at iter 500 is 0.138903\n",
            "training loss at iter 520 is 0.125554\n",
            "training loss at iter 540 is 0.140243\n",
            "training loss at iter 560 is 0.122093\n",
            "training loss at iter 580 is 0.130144\n",
            "training loss at iter 600 is 0.123377\n",
            "training loss at iter 620 is 0.128390\n",
            "training loss at iter 640 is 0.118308\n",
            "training loss at iter 660 is 0.128895\n",
            "training loss at iter 680 is 0.130390\n",
            "training loss at iter 700 is 0.139392\n",
            "training loss at iter 720 is 0.132340\n",
            "training loss at iter 740 is 0.127143\n",
            "training loss at iter 760 is 0.132775\n",
            "training loss at iter 780 is 0.135933\n",
            "training loss at iter 800 is 0.130139\n",
            "training loss at iter 820 is 0.122417\n",
            "training loss at iter 840 is 0.138394\n",
            "training loss at iter 860 is 0.125653\n",
            "training loss at iter 880 is 0.130982\n",
            "training loss at iter 900 is 0.131090\n",
            "training loss at iter 920 is 0.130766\n",
            "training loss at iter 940 is 0.124972\n",
            "training loss at iter 960 is 0.126320\n",
            "training loss at iter 980 is 0.125254\n",
            "training loss at iter 1000 is 0.145164\n",
            "0.1782\n",
            "testing NMI, ARI, ACC at epoch 18 is 0.947524, 0.930944, 0.968750.\n",
            "0.7717999999999999 0.47282\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.135843\n",
            "training loss at iter 40 is 0.128329\n",
            "training loss at iter 60 is 0.125841\n",
            "training loss at iter 80 is 0.132530\n",
            "training loss at iter 100 is 0.125329\n",
            "training loss at iter 120 is 0.128017\n",
            "training loss at iter 140 is 0.127690\n",
            "training loss at iter 160 is 0.123403\n",
            "training loss at iter 180 is 0.120809\n",
            "training loss at iter 200 is 0.122654\n",
            "training loss at iter 220 is 0.134953\n",
            "training loss at iter 240 is 0.125559\n",
            "training loss at iter 260 is 0.128503\n",
            "training loss at iter 280 is 0.144881\n",
            "training loss at iter 300 is 0.126231\n",
            "training loss at iter 320 is 0.135894\n",
            "training loss at iter 340 is 0.120066\n",
            "training loss at iter 360 is 0.126562\n",
            "training loss at iter 380 is 0.123040\n",
            "training loss at iter 400 is 0.123162\n",
            "training loss at iter 420 is 0.122046\n",
            "training loss at iter 440 is 0.135697\n",
            "training loss at iter 460 is 0.121122\n",
            "training loss at iter 480 is 0.126803\n",
            "training loss at iter 500 is 0.125254\n",
            "training loss at iter 520 is 0.135113\n",
            "training loss at iter 540 is 0.134426\n",
            "training loss at iter 560 is 0.124582\n",
            "training loss at iter 580 is 0.126556\n",
            "training loss at iter 600 is 0.125310\n",
            "training loss at iter 620 is 0.124155\n",
            "training loss at iter 640 is 0.128161\n",
            "training loss at iter 660 is 0.131352\n",
            "training loss at iter 680 is 0.135311\n",
            "training loss at iter 700 is 0.120452\n",
            "training loss at iter 720 is 0.128773\n",
            "training loss at iter 740 is 0.130459\n",
            "training loss at iter 760 is 0.122424\n",
            "training loss at iter 780 is 0.130516\n",
            "training loss at iter 800 is 0.118864\n",
            "training loss at iter 820 is 0.127110\n",
            "training loss at iter 840 is 0.128810\n",
            "training loss at iter 860 is 0.127870\n",
            "training loss at iter 880 is 0.126892\n",
            "training loss at iter 900 is 0.130774\n",
            "training loss at iter 920 is 0.129806\n",
            "training loss at iter 940 is 0.126215\n",
            "training loss at iter 960 is 0.132558\n",
            "training loss at iter 980 is 0.122150\n",
            "training loss at iter 1000 is 0.135392\n",
            "0.1881\n",
            "testing NMI, ARI, ACC at epoch 19 is 0.941185, 0.928082, 0.966797.\n",
            "0.7619 0.47381\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.128394\n",
            "training loss at iter 40 is 0.120336\n",
            "training loss at iter 60 is 0.120825\n",
            "training loss at iter 80 is 0.119447\n",
            "training loss at iter 100 is 0.126115\n",
            "training loss at iter 120 is 0.132227\n",
            "training loss at iter 140 is 0.128541\n",
            "training loss at iter 160 is 0.129762\n",
            "training loss at iter 180 is 0.123307\n",
            "training loss at iter 200 is 0.127358\n",
            "training loss at iter 220 is 0.126204\n",
            "training loss at iter 240 is 0.125295\n",
            "training loss at iter 260 is 0.138737\n",
            "training loss at iter 280 is 0.128967\n",
            "training loss at iter 300 is 0.131093\n",
            "training loss at iter 320 is 0.129292\n",
            "training loss at iter 340 is 0.132893\n",
            "training loss at iter 360 is 0.131302\n",
            "training loss at iter 380 is 0.127146\n",
            "training loss at iter 400 is 0.123191\n",
            "training loss at iter 420 is 0.127738\n",
            "training loss at iter 440 is 0.131728\n",
            "training loss at iter 460 is 0.134424\n",
            "training loss at iter 480 is 0.133524\n",
            "training loss at iter 500 is 0.126097\n",
            "training loss at iter 520 is 0.129878\n",
            "training loss at iter 540 is 0.133737\n",
            "training loss at iter 560 is 0.137914\n",
            "training loss at iter 580 is 0.121012\n",
            "training loss at iter 600 is 0.135217\n",
            "training loss at iter 620 is 0.133662\n",
            "training loss at iter 640 is 0.122409\n",
            "training loss at iter 660 is 0.134145\n",
            "training loss at iter 680 is 0.139927\n",
            "training loss at iter 700 is 0.133973\n",
            "training loss at iter 720 is 0.131011\n",
            "training loss at iter 740 is 0.126101\n",
            "training loss at iter 760 is 0.127792\n",
            "training loss at iter 780 is 0.123356\n",
            "training loss at iter 800 is 0.136687\n",
            "training loss at iter 820 is 0.120788\n",
            "training loss at iter 840 is 0.129267\n",
            "training loss at iter 860 is 0.123032\n",
            "training loss at iter 880 is 0.139865\n",
            "training loss at iter 900 is 0.131957\n",
            "training loss at iter 920 is 0.134475\n",
            "training loss at iter 940 is 0.131059\n",
            "training loss at iter 960 is 0.121662\n",
            "training loss at iter 980 is 0.125532\n",
            "training loss at iter 1000 is 0.124191\n",
            "0.19799999999999998\n",
            "testing NMI, ARI, ACC at epoch 20 is 0.934929, 0.920859, 0.962891.\n",
            "Model saved in file: DAC_models/DAC_ep_20.ckpt\n",
            "0.752 0.4748\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.132276\n",
            "training loss at iter 40 is 0.123058\n",
            "training loss at iter 60 is 0.141250\n",
            "training loss at iter 80 is 0.123798\n",
            "training loss at iter 100 is 0.132633\n",
            "training loss at iter 120 is 0.129694\n",
            "training loss at iter 140 is 0.137739\n",
            "training loss at iter 160 is 0.119653\n",
            "training loss at iter 180 is 0.127491\n",
            "training loss at iter 200 is 0.123885\n",
            "training loss at iter 220 is 0.127338\n",
            "training loss at iter 240 is 0.126021\n",
            "training loss at iter 260 is 0.120879\n",
            "training loss at iter 280 is 0.135253\n",
            "training loss at iter 300 is 0.125662\n",
            "training loss at iter 320 is 0.122279\n",
            "training loss at iter 340 is 0.120612\n",
            "training loss at iter 360 is 0.141311\n",
            "training loss at iter 380 is 0.122450\n",
            "training loss at iter 400 is 0.121293\n",
            "training loss at iter 420 is 0.124925\n",
            "training loss at iter 440 is 0.120903\n",
            "training loss at iter 460 is 0.132811\n",
            "training loss at iter 480 is 0.129554\n",
            "training loss at iter 500 is 0.125568\n",
            "training loss at iter 520 is 0.126106\n",
            "training loss at iter 540 is 0.125436\n",
            "training loss at iter 560 is 0.119396\n",
            "training loss at iter 580 is 0.125624\n",
            "training loss at iter 600 is 0.122828\n",
            "training loss at iter 620 is 0.131915\n",
            "training loss at iter 640 is 0.123309\n",
            "training loss at iter 660 is 0.130499\n",
            "training loss at iter 680 is 0.120397\n",
            "training loss at iter 700 is 0.135436\n",
            "training loss at iter 720 is 0.132078\n",
            "training loss at iter 740 is 0.124392\n",
            "training loss at iter 760 is 0.122299\n",
            "training loss at iter 780 is 0.126358\n",
            "training loss at iter 800 is 0.128398\n",
            "training loss at iter 820 is 0.123108\n",
            "training loss at iter 840 is 0.128387\n",
            "training loss at iter 860 is 0.125158\n",
            "training loss at iter 880 is 0.133050\n",
            "training loss at iter 900 is 0.122159\n",
            "training loss at iter 920 is 0.136245\n",
            "training loss at iter 940 is 0.127675\n",
            "training loss at iter 960 is 0.124580\n",
            "training loss at iter 980 is 0.127337\n",
            "training loss at iter 1000 is 0.135670\n",
            "0.20789999999999997\n",
            "testing NMI, ARI, ACC at epoch 21 is 0.936647, 0.913273, 0.962891.\n",
            "0.7421 0.47579\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.132955\n",
            "training loss at iter 40 is 0.137680\n",
            "training loss at iter 60 is 0.123415\n",
            "training loss at iter 80 is 0.119794\n",
            "training loss at iter 100 is 0.127207\n",
            "training loss at iter 120 is 0.121203\n",
            "training loss at iter 140 is 0.120458\n",
            "training loss at iter 160 is 0.144892\n",
            "training loss at iter 180 is 0.136248\n",
            "training loss at iter 200 is 0.128230\n",
            "training loss at iter 220 is 0.123909\n",
            "training loss at iter 240 is 0.127207\n",
            "training loss at iter 260 is 0.133378\n",
            "training loss at iter 280 is 0.121601\n",
            "training loss at iter 300 is 0.134115\n",
            "training loss at iter 320 is 0.127422\n",
            "training loss at iter 340 is 0.121326\n",
            "training loss at iter 360 is 0.125406\n",
            "training loss at iter 380 is 0.135910\n",
            "training loss at iter 400 is 0.127971\n",
            "training loss at iter 420 is 0.124123\n",
            "training loss at iter 440 is 0.129342\n",
            "training loss at iter 460 is 0.124583\n",
            "training loss at iter 480 is 0.139271\n",
            "training loss at iter 500 is 0.126790\n",
            "training loss at iter 520 is 0.118942\n",
            "training loss at iter 540 is 0.127015\n",
            "training loss at iter 560 is 0.125234\n",
            "training loss at iter 580 is 0.125395\n",
            "training loss at iter 600 is 0.132017\n",
            "training loss at iter 620 is 0.125653\n",
            "training loss at iter 640 is 0.126859\n",
            "training loss at iter 660 is 0.128487\n",
            "training loss at iter 680 is 0.119271\n",
            "training loss at iter 700 is 0.121118\n",
            "training loss at iter 720 is 0.138972\n",
            "training loss at iter 740 is 0.129498\n",
            "training loss at iter 760 is 0.129644\n",
            "training loss at iter 780 is 0.124588\n",
            "training loss at iter 800 is 0.128141\n",
            "training loss at iter 820 is 0.120422\n",
            "training loss at iter 840 is 0.125253\n",
            "training loss at iter 860 is 0.131371\n",
            "training loss at iter 880 is 0.126883\n",
            "training loss at iter 900 is 0.120755\n",
            "training loss at iter 920 is 0.124537\n",
            "training loss at iter 940 is 0.138911\n",
            "training loss at iter 960 is 0.126465\n",
            "training loss at iter 980 is 0.129749\n",
            "training loss at iter 1000 is 0.144210\n",
            "0.21779999999999997\n",
            "testing NMI, ARI, ACC at epoch 22 is 0.946114, 0.938233, 0.970703.\n",
            "0.7322 0.47678000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.124910\n",
            "training loss at iter 40 is 0.130417\n",
            "training loss at iter 60 is 0.122876\n",
            "training loss at iter 80 is 0.134187\n",
            "training loss at iter 100 is 0.125321\n",
            "training loss at iter 120 is 0.119619\n",
            "training loss at iter 140 is 0.118639\n",
            "training loss at iter 160 is 0.127396\n",
            "training loss at iter 180 is 0.122152\n",
            "training loss at iter 200 is 0.124273\n",
            "training loss at iter 220 is 0.122779\n",
            "training loss at iter 240 is 0.127435\n",
            "training loss at iter 260 is 0.140878\n",
            "training loss at iter 280 is 0.131060\n",
            "training loss at iter 300 is 0.123768\n",
            "training loss at iter 320 is 0.123880\n",
            "training loss at iter 340 is 0.128628\n",
            "training loss at iter 360 is 0.129832\n",
            "training loss at iter 380 is 0.124977\n",
            "training loss at iter 400 is 0.131012\n",
            "training loss at iter 420 is 0.125493\n",
            "training loss at iter 440 is 0.123906\n",
            "training loss at iter 460 is 0.125902\n",
            "training loss at iter 480 is 0.118587\n",
            "training loss at iter 500 is 0.123321\n",
            "training loss at iter 520 is 0.140725\n",
            "training loss at iter 540 is 0.120769\n",
            "training loss at iter 560 is 0.125150\n",
            "training loss at iter 580 is 0.142867\n",
            "training loss at iter 600 is 0.137697\n",
            "training loss at iter 620 is 0.125781\n",
            "training loss at iter 640 is 0.123262\n",
            "training loss at iter 660 is 0.147611\n",
            "training loss at iter 680 is 0.123334\n",
            "training loss at iter 700 is 0.126781\n",
            "training loss at iter 720 is 0.126525\n",
            "training loss at iter 740 is 0.123114\n",
            "training loss at iter 760 is 0.123134\n",
            "training loss at iter 780 is 0.123642\n",
            "training loss at iter 800 is 0.128687\n",
            "training loss at iter 820 is 0.127604\n",
            "training loss at iter 840 is 0.118872\n",
            "training loss at iter 860 is 0.122041\n",
            "training loss at iter 880 is 0.123682\n",
            "training loss at iter 900 is 0.127783\n",
            "training loss at iter 920 is 0.137860\n",
            "training loss at iter 940 is 0.115926\n",
            "training loss at iter 960 is 0.130696\n",
            "training loss at iter 980 is 0.138517\n",
            "training loss at iter 1000 is 0.143620\n",
            "0.22769999999999996\n",
            "testing NMI, ARI, ACC at epoch 23 is 0.955017, 0.932071, 0.970703.\n",
            "0.7222999999999999 0.47777000000000003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.120427\n",
            "training loss at iter 40 is 0.132948\n",
            "training loss at iter 60 is 0.121656\n",
            "training loss at iter 80 is 0.118800\n",
            "training loss at iter 100 is 0.130091\n",
            "training loss at iter 120 is 0.119275\n",
            "training loss at iter 140 is 0.137727\n",
            "training loss at iter 160 is 0.126944\n",
            "training loss at iter 180 is 0.119391\n",
            "training loss at iter 200 is 0.147194\n",
            "training loss at iter 220 is 0.132421\n",
            "training loss at iter 240 is 0.120704\n",
            "training loss at iter 260 is 0.127053\n",
            "training loss at iter 280 is 0.136078\n",
            "training loss at iter 300 is 0.126754\n",
            "training loss at iter 320 is 0.118960\n",
            "training loss at iter 340 is 0.128788\n",
            "training loss at iter 360 is 0.150039\n",
            "training loss at iter 380 is 0.129194\n",
            "training loss at iter 400 is 0.127077\n",
            "training loss at iter 420 is 0.128226\n",
            "training loss at iter 440 is 0.130736\n",
            "training loss at iter 460 is 0.124312\n",
            "training loss at iter 480 is 0.122756\n",
            "training loss at iter 500 is 0.144297\n",
            "training loss at iter 520 is 0.131240\n",
            "training loss at iter 540 is 0.122847\n",
            "training loss at iter 560 is 0.130558\n",
            "training loss at iter 580 is 0.124385\n",
            "training loss at iter 600 is 0.123107\n",
            "training loss at iter 620 is 0.124817\n",
            "training loss at iter 640 is 0.124866\n",
            "training loss at iter 660 is 0.128911\n",
            "training loss at iter 680 is 0.122087\n",
            "training loss at iter 700 is 0.124958\n",
            "training loss at iter 720 is 0.123760\n",
            "training loss at iter 740 is 0.117271\n",
            "training loss at iter 760 is 0.118503\n",
            "training loss at iter 780 is 0.125560\n",
            "training loss at iter 800 is 0.120376\n",
            "training loss at iter 820 is 0.117498\n",
            "training loss at iter 840 is 0.121080\n",
            "training loss at iter 860 is 0.124813\n",
            "training loss at iter 880 is 0.132138\n",
            "training loss at iter 900 is 0.122326\n",
            "training loss at iter 920 is 0.122770\n",
            "training loss at iter 940 is 0.121557\n",
            "training loss at iter 960 is 0.131896\n",
            "training loss at iter 980 is 0.124921\n",
            "training loss at iter 1000 is 0.119240\n",
            "0.23759999999999995\n",
            "testing NMI, ARI, ACC at epoch 24 is 0.944944, 0.929375, 0.968750.\n",
            "0.7124 0.47876\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.126521\n",
            "training loss at iter 40 is 0.125566\n",
            "training loss at iter 60 is 0.134515\n",
            "training loss at iter 80 is 0.119777\n",
            "training loss at iter 100 is 0.125110\n",
            "training loss at iter 120 is 0.124346\n",
            "training loss at iter 140 is 0.124749\n",
            "training loss at iter 160 is 0.122309\n",
            "training loss at iter 180 is 0.124558\n",
            "training loss at iter 200 is 0.127559\n",
            "training loss at iter 220 is 0.122430\n",
            "training loss at iter 240 is 0.131075\n",
            "training loss at iter 260 is 0.122575\n",
            "training loss at iter 280 is 0.117428\n",
            "training loss at iter 300 is 0.133235\n",
            "training loss at iter 320 is 0.121442\n",
            "training loss at iter 340 is 0.122783\n",
            "training loss at iter 360 is 0.138668\n",
            "training loss at iter 380 is 0.128063\n",
            "training loss at iter 400 is 0.130141\n",
            "training loss at iter 420 is 0.131019\n",
            "training loss at iter 440 is 0.122452\n",
            "training loss at iter 460 is 0.125106\n",
            "training loss at iter 480 is 0.126417\n",
            "training loss at iter 500 is 0.128245\n",
            "training loss at iter 520 is 0.126847\n",
            "training loss at iter 540 is 0.138761\n",
            "training loss at iter 560 is 0.127026\n",
            "training loss at iter 580 is 0.136902\n",
            "training loss at iter 600 is 0.139106\n",
            "training loss at iter 620 is 0.134180\n",
            "training loss at iter 640 is 0.125154\n",
            "training loss at iter 660 is 0.122118\n",
            "training loss at iter 680 is 0.122139\n",
            "training loss at iter 700 is 0.127725\n",
            "training loss at iter 720 is 0.125683\n",
            "training loss at iter 740 is 0.130930\n",
            "training loss at iter 760 is 0.122874\n",
            "training loss at iter 780 is 0.123849\n",
            "training loss at iter 800 is 0.120958\n",
            "training loss at iter 820 is 0.134103\n",
            "training loss at iter 840 is 0.132344\n",
            "training loss at iter 860 is 0.119763\n",
            "training loss at iter 880 is 0.120764\n",
            "training loss at iter 900 is 0.135037\n",
            "training loss at iter 920 is 0.134468\n",
            "training loss at iter 940 is 0.132011\n",
            "training loss at iter 960 is 0.124617\n",
            "training loss at iter 980 is 0.134427\n",
            "training loss at iter 1000 is 0.129237\n",
            "0.24749999999999994\n",
            "testing NMI, ARI, ACC at epoch 25 is 0.956255, 0.927551, 0.968750.\n",
            "Model saved in file: DAC_models/DAC_ep_25.ckpt\n",
            "0.7025 0.47975\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.118946\n",
            "training loss at iter 40 is 0.129749\n",
            "training loss at iter 60 is 0.132529\n",
            "training loss at iter 80 is 0.122783\n",
            "training loss at iter 100 is 0.123214\n",
            "training loss at iter 120 is 0.133746\n",
            "training loss at iter 140 is 0.132571\n",
            "training loss at iter 160 is 0.142254\n",
            "training loss at iter 180 is 0.125838\n",
            "training loss at iter 200 is 0.120161\n",
            "training loss at iter 220 is 0.124510\n",
            "training loss at iter 240 is 0.123064\n",
            "training loss at iter 260 is 0.124338\n",
            "training loss at iter 280 is 0.122564\n",
            "training loss at iter 300 is 0.128349\n",
            "training loss at iter 320 is 0.117434\n",
            "training loss at iter 340 is 0.139359\n",
            "training loss at iter 360 is 0.120271\n",
            "training loss at iter 380 is 0.133812\n",
            "training loss at iter 400 is 0.128055\n",
            "training loss at iter 420 is 0.134725\n",
            "training loss at iter 440 is 0.131953\n",
            "training loss at iter 460 is 0.141567\n",
            "training loss at iter 480 is 0.125198\n",
            "training loss at iter 500 is 0.140234\n",
            "training loss at iter 520 is 0.131546\n",
            "training loss at iter 540 is 0.139128\n",
            "training loss at iter 560 is 0.131120\n",
            "training loss at iter 580 is 0.129493\n",
            "training loss at iter 600 is 0.122179\n",
            "training loss at iter 620 is 0.117249\n",
            "training loss at iter 640 is 0.127635\n",
            "training loss at iter 660 is 0.121014\n",
            "training loss at iter 680 is 0.126696\n",
            "training loss at iter 700 is 0.126446\n",
            "training loss at iter 720 is 0.135010\n",
            "training loss at iter 740 is 0.125857\n",
            "training loss at iter 760 is 0.132436\n",
            "training loss at iter 780 is 0.117865\n",
            "training loss at iter 800 is 0.120612\n",
            "training loss at iter 820 is 0.125585\n",
            "training loss at iter 840 is 0.128362\n",
            "training loss at iter 860 is 0.122476\n",
            "training loss at iter 880 is 0.119209\n",
            "training loss at iter 900 is 0.126231\n",
            "training loss at iter 920 is 0.124876\n",
            "training loss at iter 940 is 0.146613\n",
            "training loss at iter 960 is 0.129556\n",
            "training loss at iter 980 is 0.126012\n",
            "training loss at iter 1000 is 0.137436\n",
            "0.25739999999999996\n",
            "testing NMI, ARI, ACC at epoch 26 is 0.948640, 0.929744, 0.968750.\n",
            "0.6926 0.48074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.142819\n",
            "training loss at iter 40 is 0.121972\n",
            "training loss at iter 60 is 0.141877\n",
            "training loss at iter 80 is 0.120998\n",
            "training loss at iter 100 is 0.127880\n",
            "training loss at iter 120 is 0.134384\n",
            "training loss at iter 140 is 0.125038\n",
            "training loss at iter 160 is 0.139308\n",
            "training loss at iter 180 is 0.132185\n",
            "training loss at iter 200 is 0.124874\n",
            "training loss at iter 220 is 0.132099\n",
            "training loss at iter 240 is 0.122313\n",
            "training loss at iter 260 is 0.137684\n",
            "training loss at iter 280 is 0.116240\n",
            "training loss at iter 300 is 0.149437\n",
            "training loss at iter 320 is 0.135646\n",
            "training loss at iter 340 is 0.128493\n",
            "training loss at iter 360 is 0.119363\n",
            "training loss at iter 380 is 0.131710\n",
            "training loss at iter 400 is 0.117335\n",
            "training loss at iter 420 is 0.128759\n",
            "training loss at iter 440 is 0.128731\n",
            "training loss at iter 460 is 0.144043\n",
            "training loss at iter 480 is 0.125175\n",
            "training loss at iter 500 is 0.143539\n",
            "training loss at iter 520 is 0.121681\n",
            "training loss at iter 540 is 0.120041\n",
            "training loss at iter 560 is 0.135602\n",
            "training loss at iter 580 is 0.127356\n",
            "training loss at iter 600 is 0.160559\n",
            "training loss at iter 620 is 0.131028\n",
            "training loss at iter 640 is 0.117160\n",
            "training loss at iter 660 is 0.120823\n",
            "training loss at iter 680 is 0.126601\n",
            "training loss at iter 700 is 0.124833\n",
            "training loss at iter 720 is 0.141596\n",
            "training loss at iter 740 is 0.135409\n",
            "training loss at iter 760 is 0.136607\n",
            "training loss at iter 780 is 0.126491\n",
            "training loss at iter 800 is 0.125088\n",
            "training loss at iter 820 is 0.113638\n",
            "training loss at iter 840 is 0.120732\n",
            "training loss at iter 860 is 0.126930\n",
            "training loss at iter 880 is 0.145390\n",
            "training loss at iter 900 is 0.119714\n",
            "training loss at iter 920 is 0.133050\n",
            "training loss at iter 940 is 0.147280\n",
            "training loss at iter 960 is 0.119441\n",
            "training loss at iter 980 is 0.128914\n",
            "training loss at iter 1000 is 0.135292\n",
            "0.2673\n",
            "testing NMI, ARI, ACC at epoch 27 is 0.968176, 0.961348, 0.982422.\n",
            "0.6827 0.48173\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.125557\n",
            "training loss at iter 40 is 0.128279\n",
            "training loss at iter 60 is 0.119397\n",
            "training loss at iter 80 is 0.125933\n",
            "training loss at iter 100 is 0.130824\n",
            "training loss at iter 120 is 0.120010\n",
            "training loss at iter 140 is 0.130821\n",
            "training loss at iter 160 is 0.125431\n",
            "training loss at iter 180 is 0.134920\n",
            "training loss at iter 200 is 0.131599\n",
            "training loss at iter 220 is 0.139199\n",
            "training loss at iter 240 is 0.126031\n",
            "training loss at iter 260 is 0.127847\n",
            "training loss at iter 280 is 0.119027\n",
            "training loss at iter 300 is 0.126661\n",
            "training loss at iter 320 is 0.124042\n",
            "training loss at iter 340 is 0.123529\n",
            "training loss at iter 360 is 0.118646\n",
            "training loss at iter 380 is 0.123886\n",
            "training loss at iter 400 is 0.142426\n",
            "training loss at iter 420 is 0.121007\n",
            "training loss at iter 440 is 0.121799\n",
            "training loss at iter 460 is 0.123580\n",
            "training loss at iter 480 is 0.125443\n",
            "training loss at iter 500 is 0.122595\n",
            "training loss at iter 520 is 0.120500\n",
            "training loss at iter 540 is 0.135025\n",
            "training loss at iter 560 is 0.122700\n",
            "training loss at iter 580 is 0.132371\n",
            "training loss at iter 600 is 0.134025\n",
            "training loss at iter 620 is 0.125711\n",
            "training loss at iter 640 is 0.127809\n",
            "training loss at iter 660 is 0.122579\n",
            "training loss at iter 680 is 0.124940\n",
            "training loss at iter 700 is 0.120255\n",
            "training loss at iter 720 is 0.136200\n",
            "training loss at iter 740 is 0.129188\n",
            "training loss at iter 760 is 0.117103\n",
            "training loss at iter 780 is 0.120557\n",
            "training loss at iter 800 is 0.128535\n",
            "training loss at iter 820 is 0.123526\n",
            "training loss at iter 840 is 0.139876\n",
            "training loss at iter 860 is 0.133145\n",
            "training loss at iter 880 is 0.128929\n",
            "training loss at iter 900 is 0.120297\n",
            "training loss at iter 920 is 0.125196\n",
            "training loss at iter 940 is 0.129699\n",
            "training loss at iter 960 is 0.127079\n",
            "training loss at iter 980 is 0.131405\n",
            "training loss at iter 1000 is 0.124112\n",
            "0.2772\n",
            "testing NMI, ARI, ACC at epoch 28 is 0.934878, 0.906285, 0.958984.\n",
            "0.6728 0.48272000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.116164\n",
            "training loss at iter 40 is 0.126537\n",
            "training loss at iter 60 is 0.126108\n",
            "training loss at iter 80 is 0.149546\n",
            "training loss at iter 100 is 0.142712\n",
            "training loss at iter 120 is 0.123409\n",
            "training loss at iter 140 is 0.128199\n",
            "training loss at iter 160 is 0.131208\n",
            "training loss at iter 180 is 0.122330\n",
            "training loss at iter 200 is 0.127175\n",
            "training loss at iter 220 is 0.121243\n",
            "training loss at iter 240 is 0.127597\n",
            "training loss at iter 260 is 0.124400\n",
            "training loss at iter 280 is 0.139678\n",
            "training loss at iter 300 is 0.135658\n",
            "training loss at iter 320 is 0.133156\n",
            "training loss at iter 340 is 0.116387\n",
            "training loss at iter 360 is 0.120204\n",
            "training loss at iter 380 is 0.125224\n",
            "training loss at iter 400 is 0.129556\n",
            "training loss at iter 420 is 0.130053\n",
            "training loss at iter 440 is 0.135616\n",
            "training loss at iter 460 is 0.129771\n",
            "training loss at iter 480 is 0.122134\n",
            "training loss at iter 500 is 0.125337\n",
            "training loss at iter 520 is 0.122965\n",
            "training loss at iter 540 is 0.133038\n",
            "training loss at iter 560 is 0.138230\n",
            "training loss at iter 580 is 0.136120\n",
            "training loss at iter 600 is 0.123703\n",
            "training loss at iter 620 is 0.124104\n",
            "training loss at iter 640 is 0.126841\n",
            "training loss at iter 660 is 0.159453\n",
            "training loss at iter 680 is 0.132352\n",
            "training loss at iter 700 is 0.123470\n",
            "training loss at iter 720 is 0.125743\n",
            "training loss at iter 740 is 0.131991\n",
            "training loss at iter 760 is 0.133328\n",
            "training loss at iter 780 is 0.127805\n",
            "training loss at iter 800 is 0.133520\n",
            "training loss at iter 820 is 0.122545\n",
            "training loss at iter 840 is 0.124664\n",
            "training loss at iter 860 is 0.120554\n",
            "training loss at iter 880 is 0.122836\n",
            "training loss at iter 900 is 0.130345\n",
            "training loss at iter 920 is 0.122838\n",
            "training loss at iter 940 is 0.132686\n",
            "training loss at iter 960 is 0.116851\n",
            "training loss at iter 980 is 0.127951\n",
            "training loss at iter 1000 is 0.125569\n",
            "0.2871\n",
            "testing NMI, ARI, ACC at epoch 29 is 0.947131, 0.934924, 0.970703.\n",
            "0.6628999999999999 0.48371000000000003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.136733\n",
            "training loss at iter 40 is 0.125913\n",
            "training loss at iter 60 is 0.130705\n",
            "training loss at iter 80 is 0.132840\n",
            "training loss at iter 100 is 0.123787\n",
            "training loss at iter 120 is 0.139781\n",
            "training loss at iter 140 is 0.125017\n",
            "training loss at iter 160 is 0.128248\n",
            "training loss at iter 180 is 0.129193\n",
            "training loss at iter 200 is 0.118412\n",
            "training loss at iter 220 is 0.128402\n",
            "training loss at iter 240 is 0.118905\n",
            "training loss at iter 260 is 0.126768\n",
            "training loss at iter 280 is 0.120412\n",
            "training loss at iter 300 is 0.129370\n",
            "training loss at iter 320 is 0.128144\n",
            "training loss at iter 340 is 0.138774\n",
            "training loss at iter 360 is 0.126285\n",
            "training loss at iter 380 is 0.126099\n",
            "training loss at iter 400 is 0.145040\n",
            "training loss at iter 420 is 0.138489\n",
            "training loss at iter 440 is 0.125013\n",
            "training loss at iter 460 is 0.132676\n",
            "training loss at iter 480 is 0.123562\n",
            "training loss at iter 500 is 0.127852\n",
            "training loss at iter 520 is 0.135669\n",
            "training loss at iter 540 is 0.132610\n",
            "training loss at iter 560 is 0.122207\n",
            "training loss at iter 580 is 0.137142\n",
            "training loss at iter 600 is 0.117891\n",
            "training loss at iter 620 is 0.124895\n",
            "training loss at iter 640 is 0.132599\n",
            "training loss at iter 660 is 0.123490\n",
            "training loss at iter 680 is 0.126219\n",
            "training loss at iter 700 is 0.141281\n",
            "training loss at iter 720 is 0.123876\n",
            "training loss at iter 740 is 0.136635\n",
            "training loss at iter 760 is 0.142070\n",
            "training loss at iter 780 is 0.122939\n",
            "training loss at iter 800 is 0.137124\n",
            "training loss at iter 820 is 0.144489\n",
            "training loss at iter 840 is 0.122954\n",
            "training loss at iter 860 is 0.136663\n",
            "training loss at iter 880 is 0.118708\n",
            "training loss at iter 900 is 0.118406\n",
            "training loss at iter 920 is 0.123577\n",
            "training loss at iter 940 is 0.124906\n",
            "training loss at iter 960 is 0.127377\n",
            "training loss at iter 980 is 0.122187\n",
            "training loss at iter 1000 is 0.115940\n",
            "0.29700000000000004\n",
            "testing NMI, ARI, ACC at epoch 30 is 0.933532, 0.918859, 0.960938.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:971: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "Model saved in file: DAC_models/DAC_ep_30.ckpt\n",
            "0.6529999999999999 0.4847\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.127339\n",
            "training loss at iter 40 is 0.122559\n",
            "training loss at iter 60 is 0.129475\n",
            "training loss at iter 80 is 0.117295\n",
            "training loss at iter 100 is 0.128350\n",
            "training loss at iter 120 is 0.124361\n",
            "training loss at iter 140 is 0.125163\n",
            "training loss at iter 160 is 0.120244\n",
            "training loss at iter 180 is 0.126237\n",
            "training loss at iter 200 is 0.136472\n",
            "training loss at iter 220 is 0.138269\n",
            "training loss at iter 240 is 0.129522\n",
            "training loss at iter 260 is 0.131960\n",
            "training loss at iter 280 is 0.127925\n",
            "training loss at iter 300 is 0.120832\n",
            "training loss at iter 320 is 0.139045\n",
            "training loss at iter 340 is 0.121047\n",
            "training loss at iter 360 is 0.134363\n",
            "training loss at iter 380 is 0.136512\n",
            "training loss at iter 400 is 0.133752\n",
            "training loss at iter 420 is 0.124649\n",
            "training loss at iter 440 is 0.133659\n",
            "training loss at iter 460 is 0.127433\n",
            "training loss at iter 480 is 0.140401\n",
            "training loss at iter 500 is 0.131892\n",
            "training loss at iter 520 is 0.136358\n",
            "training loss at iter 540 is 0.134370\n",
            "training loss at iter 560 is 0.120947\n",
            "training loss at iter 580 is 0.128421\n",
            "training loss at iter 600 is 0.138035\n",
            "training loss at iter 620 is 0.124972\n",
            "training loss at iter 640 is 0.129229\n",
            "training loss at iter 660 is 0.123897\n",
            "training loss at iter 680 is 0.146040\n",
            "training loss at iter 700 is 0.121743\n",
            "training loss at iter 720 is 0.123867\n",
            "training loss at iter 740 is 0.120824\n",
            "training loss at iter 760 is 0.121530\n",
            "training loss at iter 780 is 0.127903\n",
            "training loss at iter 800 is 0.130508\n",
            "training loss at iter 820 is 0.137343\n",
            "training loss at iter 840 is 0.124311\n",
            "training loss at iter 860 is 0.126833\n",
            "training loss at iter 880 is 0.117497\n",
            "training loss at iter 900 is 0.115827\n",
            "training loss at iter 920 is 0.133508\n",
            "training loss at iter 940 is 0.146602\n",
            "training loss at iter 960 is 0.121139\n",
            "training loss at iter 980 is 0.129600\n",
            "training loss at iter 1000 is 0.138900\n",
            "0.30690000000000006\n",
            "testing NMI, ARI, ACC at epoch 31 is 0.959130, 0.945443, 0.974609.\n",
            "0.6430999999999999 0.48569\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.122353\n",
            "training loss at iter 40 is 0.121949\n",
            "training loss at iter 60 is 0.125997\n",
            "training loss at iter 80 is 0.116829\n",
            "training loss at iter 100 is 0.123035\n",
            "training loss at iter 120 is 0.120565\n",
            "training loss at iter 140 is 0.120669\n",
            "training loss at iter 160 is 0.129316\n",
            "training loss at iter 180 is 0.134205\n",
            "training loss at iter 200 is 0.125987\n",
            "training loss at iter 220 is 0.129841\n",
            "training loss at iter 240 is 0.114534\n",
            "training loss at iter 260 is 0.127509\n",
            "training loss at iter 280 is 0.123134\n",
            "training loss at iter 300 is 0.123745\n",
            "training loss at iter 320 is 0.128173\n",
            "training loss at iter 340 is 0.129438\n",
            "training loss at iter 360 is 0.123866\n",
            "training loss at iter 380 is 0.122210\n",
            "training loss at iter 400 is 0.125347\n",
            "training loss at iter 420 is 0.126297\n",
            "training loss at iter 440 is 0.122649\n",
            "training loss at iter 460 is 0.138758\n",
            "training loss at iter 480 is 0.125150\n",
            "training loss at iter 500 is 0.134652\n",
            "training loss at iter 520 is 0.129420\n",
            "training loss at iter 540 is 0.116698\n",
            "training loss at iter 560 is 0.131098\n",
            "training loss at iter 580 is 0.135584\n",
            "training loss at iter 600 is 0.137799\n",
            "training loss at iter 620 is 0.125617\n",
            "training loss at iter 640 is 0.129497\n",
            "training loss at iter 660 is 0.129018\n",
            "training loss at iter 680 is 0.126495\n",
            "training loss at iter 700 is 0.126148\n",
            "training loss at iter 720 is 0.118972\n",
            "training loss at iter 740 is 0.126883\n",
            "training loss at iter 760 is 0.117897\n",
            "training loss at iter 780 is 0.120172\n",
            "training loss at iter 800 is 0.121015\n",
            "training loss at iter 820 is 0.124225\n",
            "training loss at iter 840 is 0.118954\n",
            "training loss at iter 860 is 0.127610\n",
            "training loss at iter 880 is 0.135403\n",
            "training loss at iter 900 is 0.131511\n",
            "training loss at iter 920 is 0.123092\n",
            "training loss at iter 940 is 0.129817\n",
            "training loss at iter 960 is 0.121972\n",
            "training loss at iter 980 is 0.123658\n",
            "training loss at iter 1000 is 0.121386\n",
            "0.3168000000000001\n",
            "testing NMI, ARI, ACC at epoch 32 is 0.959139, 0.942180, 0.972656.\n",
            "0.6331999999999999 0.48668\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.116394\n",
            "training loss at iter 40 is 0.117764\n",
            "training loss at iter 60 is 0.134536\n",
            "training loss at iter 80 is 0.126088\n",
            "training loss at iter 100 is 0.144857\n",
            "training loss at iter 120 is 0.124253\n",
            "training loss at iter 140 is 0.117943\n",
            "training loss at iter 160 is 0.128637\n",
            "training loss at iter 180 is 0.127034\n",
            "training loss at iter 200 is 0.131855\n",
            "training loss at iter 220 is 0.126779\n",
            "training loss at iter 240 is 0.127566\n",
            "training loss at iter 260 is 0.127415\n",
            "training loss at iter 280 is 0.128494\n",
            "training loss at iter 300 is 0.124818\n",
            "training loss at iter 320 is 0.140390\n",
            "training loss at iter 340 is 0.122642\n",
            "training loss at iter 360 is 0.138648\n",
            "training loss at iter 380 is 0.125075\n",
            "training loss at iter 400 is 0.120623\n",
            "training loss at iter 420 is 0.132861\n",
            "training loss at iter 440 is 0.136219\n",
            "training loss at iter 460 is 0.138048\n",
            "training loss at iter 480 is 0.125042\n",
            "training loss at iter 500 is 0.123114\n",
            "training loss at iter 520 is 0.129982\n",
            "training loss at iter 540 is 0.132982\n",
            "training loss at iter 560 is 0.133974\n",
            "training loss at iter 580 is 0.122319\n",
            "training loss at iter 600 is 0.124046\n",
            "training loss at iter 620 is 0.123570\n",
            "training loss at iter 640 is 0.130944\n",
            "training loss at iter 660 is 0.137162\n",
            "training loss at iter 680 is 0.123596\n",
            "training loss at iter 700 is 0.136983\n",
            "training loss at iter 720 is 0.124003\n",
            "training loss at iter 740 is 0.130741\n",
            "training loss at iter 760 is 0.153463\n",
            "training loss at iter 780 is 0.138271\n",
            "training loss at iter 800 is 0.126353\n",
            "training loss at iter 820 is 0.131925\n",
            "training loss at iter 840 is 0.128051\n",
            "training loss at iter 860 is 0.127245\n",
            "training loss at iter 880 is 0.125710\n",
            "training loss at iter 900 is 0.131784\n",
            "training loss at iter 920 is 0.134022\n",
            "training loss at iter 940 is 0.127621\n",
            "training loss at iter 960 is 0.127873\n",
            "training loss at iter 980 is 0.127819\n",
            "training loss at iter 1000 is 0.134052\n",
            "0.3267000000000001\n",
            "testing NMI, ARI, ACC at epoch 33 is 0.949606, 0.939776, 0.972656.\n",
            "0.6232999999999999 0.48767000000000005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.119156\n",
            "training loss at iter 40 is 0.120070\n",
            "training loss at iter 60 is 0.133969\n",
            "training loss at iter 80 is 0.132461\n",
            "training loss at iter 100 is 0.124625\n",
            "training loss at iter 120 is 0.127459\n",
            "training loss at iter 140 is 0.124264\n",
            "training loss at iter 160 is 0.126365\n",
            "training loss at iter 180 is 0.132183\n",
            "training loss at iter 200 is 0.133942\n",
            "training loss at iter 220 is 0.127537\n",
            "training loss at iter 240 is 0.130214\n",
            "training loss at iter 260 is 0.127390\n",
            "training loss at iter 280 is 0.128690\n",
            "training loss at iter 300 is 0.121393\n",
            "training loss at iter 320 is 0.124487\n",
            "training loss at iter 340 is 0.125958\n",
            "training loss at iter 360 is 0.133992\n",
            "training loss at iter 380 is 0.127750\n",
            "training loss at iter 400 is 0.127154\n",
            "training loss at iter 420 is 0.147970\n",
            "training loss at iter 440 is 0.126218\n",
            "training loss at iter 460 is 0.127810\n",
            "training loss at iter 480 is 0.127744\n",
            "training loss at iter 500 is 0.121297\n",
            "training loss at iter 520 is 0.115924\n",
            "training loss at iter 540 is 0.125802\n",
            "training loss at iter 560 is 0.131576\n",
            "training loss at iter 580 is 0.126187\n",
            "training loss at iter 600 is 0.117195\n",
            "training loss at iter 620 is 0.135128\n",
            "training loss at iter 640 is 0.123429\n",
            "training loss at iter 660 is 0.125042\n",
            "training loss at iter 680 is 0.137052\n",
            "training loss at iter 700 is 0.127035\n",
            "training loss at iter 720 is 0.154652\n",
            "training loss at iter 740 is 0.126212\n",
            "training loss at iter 760 is 0.126653\n",
            "training loss at iter 780 is 0.125213\n",
            "training loss at iter 800 is 0.129539\n",
            "training loss at iter 820 is 0.129314\n",
            "training loss at iter 840 is 0.120329\n",
            "training loss at iter 860 is 0.125036\n",
            "training loss at iter 880 is 0.130381\n",
            "training loss at iter 900 is 0.126287\n",
            "training loss at iter 920 is 0.121140\n",
            "training loss at iter 940 is 0.126565\n",
            "training loss at iter 960 is 0.121778\n",
            "training loss at iter 980 is 0.139756\n",
            "training loss at iter 1000 is 0.124382\n",
            "0.3366000000000001\n",
            "testing NMI, ARI, ACC at epoch 34 is 0.942115, 0.922991, 0.966797.\n",
            "0.6133999999999998 0.48866000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.122163\n",
            "training loss at iter 40 is 0.115862\n",
            "training loss at iter 60 is 0.132428\n",
            "training loss at iter 80 is 0.123874\n",
            "training loss at iter 100 is 0.129812\n",
            "training loss at iter 120 is 0.127136\n",
            "training loss at iter 140 is 0.121279\n",
            "training loss at iter 160 is 0.131525\n",
            "training loss at iter 180 is 0.117128\n",
            "training loss at iter 200 is 0.122649\n",
            "training loss at iter 220 is 0.121759\n",
            "training loss at iter 240 is 0.130105\n",
            "training loss at iter 260 is 0.122944\n",
            "training loss at iter 280 is 0.128238\n",
            "training loss at iter 300 is 0.125292\n",
            "training loss at iter 320 is 0.123593\n",
            "training loss at iter 340 is 0.126822\n",
            "training loss at iter 360 is 0.125702\n",
            "training loss at iter 380 is 0.124187\n",
            "training loss at iter 400 is 0.135010\n",
            "training loss at iter 420 is 0.130103\n",
            "training loss at iter 440 is 0.119543\n",
            "training loss at iter 460 is 0.128383\n",
            "training loss at iter 480 is 0.124317\n",
            "training loss at iter 500 is 0.127376\n",
            "training loss at iter 520 is 0.128888\n",
            "training loss at iter 540 is 0.121556\n",
            "training loss at iter 560 is 0.121467\n",
            "training loss at iter 580 is 0.127234\n",
            "training loss at iter 600 is 0.125688\n",
            "training loss at iter 620 is 0.124432\n",
            "training loss at iter 640 is 0.124947\n",
            "training loss at iter 660 is 0.133826\n",
            "training loss at iter 680 is 0.132618\n",
            "training loss at iter 700 is 0.127908\n",
            "training loss at iter 720 is 0.135782\n",
            "training loss at iter 740 is 0.132641\n",
            "training loss at iter 760 is 0.122677\n",
            "training loss at iter 780 is 0.131253\n",
            "training loss at iter 800 is 0.128912\n",
            "training loss at iter 820 is 0.124479\n",
            "training loss at iter 840 is 0.129427\n",
            "training loss at iter 860 is 0.133239\n",
            "training loss at iter 880 is 0.131396\n",
            "training loss at iter 900 is 0.132048\n",
            "training loss at iter 920 is 0.127322\n",
            "training loss at iter 940 is 0.122667\n",
            "training loss at iter 960 is 0.128165\n",
            "training loss at iter 980 is 0.144988\n",
            "training loss at iter 1000 is 0.129110\n",
            "0.34650000000000014\n",
            "testing NMI, ARI, ACC at epoch 35 is 0.943164, 0.932233, 0.970703.\n",
            "Model saved in file: DAC_models/DAC_ep_35.ckpt\n",
            "0.6034999999999998 0.48965000000000003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.120617\n",
            "training loss at iter 40 is 0.127891\n",
            "training loss at iter 60 is 0.130405\n",
            "training loss at iter 80 is 0.147121\n",
            "training loss at iter 100 is 0.119048\n",
            "training loss at iter 120 is 0.121016\n",
            "training loss at iter 140 is 0.126163\n",
            "training loss at iter 160 is 0.122742\n",
            "training loss at iter 180 is 0.128868\n",
            "training loss at iter 200 is 0.120607\n",
            "training loss at iter 220 is 0.120949\n",
            "training loss at iter 240 is 0.126630\n",
            "training loss at iter 260 is 0.138684\n",
            "training loss at iter 280 is 0.120011\n",
            "training loss at iter 300 is 0.122082\n",
            "training loss at iter 320 is 0.132053\n",
            "training loss at iter 340 is 0.139174\n",
            "training loss at iter 360 is 0.126652\n",
            "training loss at iter 380 is 0.125399\n",
            "training loss at iter 400 is 0.130912\n",
            "training loss at iter 420 is 0.123734\n",
            "training loss at iter 440 is 0.127584\n",
            "training loss at iter 460 is 0.135794\n",
            "training loss at iter 480 is 0.121089\n",
            "training loss at iter 500 is 0.128353\n",
            "training loss at iter 520 is 0.129162\n",
            "training loss at iter 540 is 0.121048\n",
            "training loss at iter 560 is 0.127735\n",
            "training loss at iter 580 is 0.132290\n",
            "training loss at iter 600 is 0.123248\n",
            "training loss at iter 620 is 0.128627\n",
            "training loss at iter 640 is 0.126631\n",
            "training loss at iter 660 is 0.138931\n",
            "training loss at iter 680 is 0.118728\n",
            "training loss at iter 700 is 0.131393\n",
            "training loss at iter 720 is 0.130922\n",
            "training loss at iter 740 is 0.129418\n",
            "training loss at iter 760 is 0.132863\n",
            "training loss at iter 780 is 0.127423\n",
            "training loss at iter 800 is 0.131951\n",
            "training loss at iter 820 is 0.131441\n",
            "training loss at iter 840 is 0.127367\n",
            "training loss at iter 860 is 0.118234\n",
            "training loss at iter 880 is 0.125684\n",
            "training loss at iter 900 is 0.123476\n",
            "training loss at iter 920 is 0.123968\n",
            "training loss at iter 940 is 0.126989\n",
            "training loss at iter 960 is 0.126377\n",
            "training loss at iter 980 is 0.126802\n",
            "training loss at iter 1000 is 0.127161\n",
            "0.35640000000000016\n",
            "testing NMI, ARI, ACC at epoch 36 is 0.967405, 0.964093, 0.984375.\n",
            "0.5935999999999998 0.49064\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.156672\n",
            "training loss at iter 40 is 0.122234\n",
            "training loss at iter 60 is 0.125575\n",
            "training loss at iter 80 is 0.124918\n",
            "training loss at iter 100 is 0.125340\n",
            "training loss at iter 120 is 0.125372\n",
            "training loss at iter 140 is 0.128994\n",
            "training loss at iter 160 is 0.120671\n",
            "training loss at iter 180 is 0.122437\n",
            "training loss at iter 200 is 0.134701\n",
            "training loss at iter 220 is 0.121940\n",
            "training loss at iter 240 is 0.122409\n",
            "training loss at iter 260 is 0.135492\n",
            "training loss at iter 280 is 0.132878\n",
            "training loss at iter 300 is 0.135077\n",
            "training loss at iter 320 is 0.127150\n",
            "training loss at iter 340 is 0.118382\n",
            "training loss at iter 360 is 0.124281\n",
            "training loss at iter 380 is 0.126815\n",
            "training loss at iter 400 is 0.120459\n",
            "training loss at iter 420 is 0.124274\n",
            "training loss at iter 440 is 0.127068\n",
            "training loss at iter 460 is 0.118419\n",
            "training loss at iter 480 is 0.142171\n",
            "training loss at iter 500 is 0.120519\n",
            "training loss at iter 520 is 0.128876\n",
            "training loss at iter 540 is 0.120614\n",
            "training loss at iter 560 is 0.125773\n",
            "training loss at iter 580 is 0.123391\n",
            "training loss at iter 600 is 0.119375\n",
            "training loss at iter 620 is 0.120319\n",
            "training loss at iter 640 is 0.127487\n",
            "training loss at iter 660 is 0.121033\n",
            "training loss at iter 680 is 0.124275\n",
            "training loss at iter 700 is 0.120616\n",
            "training loss at iter 720 is 0.125304\n",
            "training loss at iter 740 is 0.139533\n",
            "training loss at iter 760 is 0.121786\n",
            "training loss at iter 780 is 0.131925\n",
            "training loss at iter 800 is 0.120248\n",
            "training loss at iter 820 is 0.125458\n",
            "training loss at iter 840 is 0.130300\n",
            "training loss at iter 860 is 0.134676\n",
            "training loss at iter 880 is 0.124253\n",
            "training loss at iter 900 is 0.130818\n",
            "training loss at iter 920 is 0.125463\n",
            "training loss at iter 940 is 0.120107\n",
            "training loss at iter 960 is 0.122361\n",
            "training loss at iter 980 is 0.128120\n",
            "training loss at iter 1000 is 0.139722\n",
            "0.3663000000000002\n",
            "testing NMI, ARI, ACC at epoch 37 is 0.957821, 0.953601, 0.978516.\n",
            "0.5836999999999998 0.49163\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.119952\n",
            "training loss at iter 40 is 0.126303\n",
            "training loss at iter 60 is 0.126644\n",
            "training loss at iter 80 is 0.140025\n",
            "training loss at iter 100 is 0.132144\n",
            "training loss at iter 120 is 0.125094\n",
            "training loss at iter 140 is 0.125660\n",
            "training loss at iter 160 is 0.131821\n",
            "training loss at iter 180 is 0.122889\n",
            "training loss at iter 200 is 0.122897\n",
            "training loss at iter 220 is 0.137489\n",
            "training loss at iter 240 is 0.122612\n",
            "training loss at iter 260 is 0.127779\n",
            "training loss at iter 280 is 0.126767\n",
            "training loss at iter 300 is 0.124220\n",
            "training loss at iter 320 is 0.134352\n",
            "training loss at iter 340 is 0.115383\n",
            "training loss at iter 360 is 0.127776\n",
            "training loss at iter 380 is 0.119993\n",
            "training loss at iter 400 is 0.117376\n",
            "training loss at iter 420 is 0.137362\n",
            "training loss at iter 440 is 0.121539\n",
            "training loss at iter 460 is 0.137577\n",
            "training loss at iter 480 is 0.128417\n",
            "training loss at iter 500 is 0.121121\n",
            "training loss at iter 520 is 0.129167\n",
            "training loss at iter 540 is 0.117274\n",
            "training loss at iter 560 is 0.120193\n",
            "training loss at iter 580 is 0.135137\n",
            "training loss at iter 600 is 0.129988\n",
            "training loss at iter 620 is 0.122703\n",
            "training loss at iter 640 is 0.131899\n",
            "training loss at iter 660 is 0.125373\n",
            "training loss at iter 680 is 0.129122\n",
            "training loss at iter 700 is 0.119374\n",
            "training loss at iter 720 is 0.129462\n",
            "training loss at iter 740 is 0.122258\n",
            "training loss at iter 760 is 0.123080\n",
            "training loss at iter 780 is 0.132235\n",
            "training loss at iter 800 is 0.117887\n",
            "training loss at iter 820 is 0.131503\n",
            "training loss at iter 840 is 0.127782\n",
            "training loss at iter 860 is 0.141643\n",
            "training loss at iter 880 is 0.122483\n",
            "training loss at iter 900 is 0.115341\n",
            "training loss at iter 920 is 0.136702\n",
            "training loss at iter 940 is 0.150076\n",
            "training loss at iter 960 is 0.121944\n",
            "training loss at iter 980 is 0.127338\n",
            "training loss at iter 1000 is 0.118147\n",
            "0.3762000000000002\n",
            "testing NMI, ARI, ACC at epoch 38 is 0.941970, 0.933881, 0.968750.\n",
            "0.5737999999999998 0.49262000000000006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.122432\n",
            "training loss at iter 40 is 0.127353\n",
            "training loss at iter 60 is 0.125770\n",
            "training loss at iter 80 is 0.126500\n",
            "training loss at iter 100 is 0.117587\n",
            "training loss at iter 120 is 0.137821\n",
            "training loss at iter 140 is 0.122996\n",
            "training loss at iter 160 is 0.118553\n",
            "training loss at iter 180 is 0.124776\n",
            "training loss at iter 200 is 0.138188\n",
            "training loss at iter 220 is 0.125490\n",
            "training loss at iter 240 is 0.133963\n",
            "training loss at iter 260 is 0.124372\n",
            "training loss at iter 280 is 0.122285\n",
            "training loss at iter 300 is 0.132872\n",
            "training loss at iter 320 is 0.115959\n",
            "training loss at iter 340 is 0.123482\n",
            "training loss at iter 360 is 0.136896\n",
            "training loss at iter 380 is 0.120854\n",
            "training loss at iter 400 is 0.122350\n",
            "training loss at iter 420 is 0.120002\n",
            "training loss at iter 440 is 0.120473\n",
            "training loss at iter 460 is 0.131189\n",
            "training loss at iter 480 is 0.124928\n",
            "training loss at iter 500 is 0.125148\n",
            "training loss at iter 520 is 0.126810\n",
            "training loss at iter 540 is 0.160815\n",
            "training loss at iter 560 is 0.119500\n",
            "training loss at iter 580 is 0.122494\n",
            "training loss at iter 600 is 0.119215\n",
            "training loss at iter 620 is 0.131282\n",
            "training loss at iter 640 is 0.124160\n",
            "training loss at iter 660 is 0.127771\n",
            "training loss at iter 680 is 0.130782\n",
            "training loss at iter 700 is 0.130845\n",
            "training loss at iter 720 is 0.125671\n",
            "training loss at iter 740 is 0.132380\n",
            "training loss at iter 760 is 0.125028\n",
            "training loss at iter 780 is 0.123998\n",
            "training loss at iter 800 is 0.130849\n",
            "training loss at iter 820 is 0.127778\n",
            "training loss at iter 840 is 0.122552\n",
            "training loss at iter 860 is 0.126194\n",
            "training loss at iter 880 is 0.129366\n",
            "training loss at iter 900 is 0.132982\n",
            "training loss at iter 920 is 0.119436\n",
            "training loss at iter 940 is 0.129028\n",
            "training loss at iter 960 is 0.128992\n",
            "training loss at iter 980 is 0.128904\n",
            "training loss at iter 1000 is 0.130163\n",
            "0.3861000000000002\n",
            "testing NMI, ARI, ACC at epoch 39 is 0.977084, 0.968435, 0.986328.\n",
            "0.5638999999999997 0.49361000000000005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.121542\n",
            "training loss at iter 40 is 0.129343\n",
            "training loss at iter 60 is 0.128764\n",
            "training loss at iter 80 is 0.130060\n",
            "training loss at iter 100 is 0.121559\n",
            "training loss at iter 120 is 0.121541\n",
            "training loss at iter 140 is 0.123593\n",
            "training loss at iter 160 is 0.127657\n",
            "training loss at iter 180 is 0.130426\n",
            "training loss at iter 200 is 0.120532\n",
            "training loss at iter 220 is 0.131919\n",
            "training loss at iter 240 is 0.123756\n",
            "training loss at iter 260 is 0.131524\n",
            "training loss at iter 280 is 0.127657\n",
            "training loss at iter 300 is 0.129132\n",
            "training loss at iter 320 is 0.126231\n",
            "training loss at iter 340 is 0.128473\n",
            "training loss at iter 360 is 0.124657\n",
            "training loss at iter 380 is 0.118546\n",
            "training loss at iter 400 is 0.129608\n",
            "training loss at iter 420 is 0.136236\n",
            "training loss at iter 440 is 0.120811\n",
            "training loss at iter 460 is 0.122109\n",
            "training loss at iter 480 is 0.135232\n",
            "training loss at iter 500 is 0.146557\n",
            "training loss at iter 520 is 0.121919\n",
            "training loss at iter 540 is 0.124031\n",
            "training loss at iter 560 is 0.134501\n",
            "training loss at iter 580 is 0.129067\n",
            "training loss at iter 600 is 0.121844\n",
            "training loss at iter 620 is 0.125989\n",
            "training loss at iter 640 is 0.127405\n",
            "training loss at iter 660 is 0.121328\n",
            "training loss at iter 680 is 0.126894\n",
            "training loss at iter 700 is 0.130834\n",
            "training loss at iter 720 is 0.127121\n",
            "training loss at iter 740 is 0.127489\n",
            "training loss at iter 760 is 0.142900\n",
            "training loss at iter 780 is 0.128313\n",
            "training loss at iter 800 is 0.122120\n",
            "training loss at iter 820 is 0.128958\n",
            "training loss at iter 840 is 0.127465\n",
            "training loss at iter 860 is 0.120739\n",
            "training loss at iter 880 is 0.120604\n",
            "training loss at iter 900 is 0.130480\n",
            "training loss at iter 920 is 0.124234\n",
            "training loss at iter 940 is 0.130987\n",
            "training loss at iter 960 is 0.119564\n",
            "training loss at iter 980 is 0.134403\n",
            "training loss at iter 1000 is 0.137724\n",
            "0.39600000000000024\n",
            "testing NMI, ARI, ACC at epoch 40 is 0.944534, 0.932312, 0.968750.\n",
            "Model saved in file: DAC_models/DAC_ep_40.ckpt\n",
            "0.5539999999999997 0.49460000000000004\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.144861\n",
            "training loss at iter 40 is 0.125961\n",
            "training loss at iter 60 is 0.128124\n",
            "training loss at iter 80 is 0.129425\n",
            "training loss at iter 100 is 0.121138\n",
            "training loss at iter 120 is 0.125897\n",
            "training loss at iter 140 is 0.125020\n",
            "training loss at iter 160 is 0.130568\n",
            "training loss at iter 180 is 0.126731\n",
            "training loss at iter 200 is 0.124210\n",
            "training loss at iter 220 is 0.130433\n",
            "training loss at iter 240 is 0.120405\n",
            "training loss at iter 260 is 0.122790\n",
            "training loss at iter 280 is 0.126982\n",
            "training loss at iter 300 is 0.126156\n",
            "training loss at iter 320 is 0.122690\n",
            "training loss at iter 340 is 0.121245\n",
            "training loss at iter 360 is 0.122510\n",
            "training loss at iter 380 is 0.130138\n",
            "training loss at iter 400 is 0.134210\n",
            "training loss at iter 420 is 0.128147\n",
            "training loss at iter 440 is 0.122218\n",
            "training loss at iter 460 is 0.119414\n",
            "training loss at iter 480 is 0.119480\n",
            "training loss at iter 500 is 0.129402\n",
            "training loss at iter 520 is 0.131395\n",
            "training loss at iter 540 is 0.130569\n",
            "training loss at iter 560 is 0.131752\n",
            "training loss at iter 580 is 0.124996\n",
            "training loss at iter 600 is 0.130543\n",
            "training loss at iter 620 is 0.118485\n",
            "training loss at iter 640 is 0.127258\n",
            "training loss at iter 660 is 0.132848\n",
            "training loss at iter 680 is 0.126996\n",
            "training loss at iter 700 is 0.126965\n",
            "training loss at iter 720 is 0.125705\n",
            "training loss at iter 740 is 0.126202\n",
            "training loss at iter 760 is 0.122981\n",
            "training loss at iter 780 is 0.121140\n",
            "training loss at iter 800 is 0.136135\n",
            "training loss at iter 820 is 0.123993\n",
            "training loss at iter 840 is 0.117612\n",
            "training loss at iter 860 is 0.134147\n",
            "training loss at iter 880 is 0.127180\n",
            "training loss at iter 900 is 0.127890\n",
            "training loss at iter 920 is 0.140480\n",
            "training loss at iter 940 is 0.125285\n",
            "training loss at iter 960 is 0.120018\n",
            "training loss at iter 980 is 0.129407\n",
            "training loss at iter 1000 is 0.130195\n",
            "0.40590000000000026\n",
            "testing NMI, ARI, ACC at epoch 41 is 0.945876, 0.934216, 0.968750.\n",
            "0.5440999999999997 0.49559000000000003\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.122288\n",
            "training loss at iter 40 is 0.124884\n",
            "training loss at iter 60 is 0.125614\n",
            "training loss at iter 80 is 0.125506\n",
            "training loss at iter 100 is 0.144443\n",
            "training loss at iter 120 is 0.127280\n",
            "training loss at iter 140 is 0.126564\n",
            "training loss at iter 160 is 0.130970\n",
            "training loss at iter 180 is 0.137139\n",
            "training loss at iter 200 is 0.128093\n",
            "training loss at iter 220 is 0.120977\n",
            "training loss at iter 240 is 0.132090\n",
            "training loss at iter 260 is 0.123626\n",
            "training loss at iter 280 is 0.124049\n",
            "training loss at iter 300 is 0.135184\n",
            "training loss at iter 320 is 0.124703\n",
            "training loss at iter 340 is 0.127752\n",
            "training loss at iter 360 is 0.131831\n",
            "training loss at iter 380 is 0.129341\n",
            "training loss at iter 400 is 0.134482\n",
            "training loss at iter 420 is 0.128009\n",
            "training loss at iter 440 is 0.127267\n",
            "training loss at iter 460 is 0.118438\n",
            "training loss at iter 480 is 0.119309\n",
            "training loss at iter 500 is 0.132619\n",
            "training loss at iter 520 is 0.131645\n",
            "training loss at iter 540 is 0.139099\n",
            "training loss at iter 560 is 0.121296\n",
            "training loss at iter 580 is 0.126868\n",
            "training loss at iter 600 is 0.123269\n",
            "training loss at iter 620 is 0.124764\n",
            "training loss at iter 640 is 0.119918\n",
            "training loss at iter 660 is 0.123513\n",
            "training loss at iter 680 is 0.126354\n",
            "training loss at iter 700 is 0.122756\n",
            "training loss at iter 720 is 0.119321\n",
            "training loss at iter 740 is 0.122980\n",
            "training loss at iter 760 is 0.126810\n",
            "training loss at iter 780 is 0.116936\n",
            "training loss at iter 800 is 0.121681\n",
            "training loss at iter 820 is 0.128685\n",
            "training loss at iter 840 is 0.127860\n",
            "training loss at iter 860 is 0.143854\n",
            "training loss at iter 880 is 0.120472\n",
            "training loss at iter 900 is 0.146711\n",
            "training loss at iter 920 is 0.120838\n",
            "training loss at iter 940 is 0.142795\n",
            "training loss at iter 960 is 0.126388\n",
            "training loss at iter 980 is 0.119741\n",
            "training loss at iter 1000 is 0.125916\n",
            "0.4158000000000003\n",
            "testing NMI, ARI, ACC at epoch 42 is 0.963420, 0.961196, 0.982422.\n",
            "0.5341999999999997 0.49658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.121480\n",
            "training loss at iter 40 is 0.125213\n",
            "training loss at iter 60 is 0.123759\n",
            "training loss at iter 80 is 0.123785\n",
            "training loss at iter 100 is 0.127526\n",
            "training loss at iter 120 is 0.136617\n",
            "training loss at iter 140 is 0.120562\n",
            "training loss at iter 160 is 0.123030\n",
            "training loss at iter 180 is 0.121579\n",
            "training loss at iter 200 is 0.130361\n",
            "training loss at iter 220 is 0.124349\n",
            "training loss at iter 240 is 0.137417\n",
            "training loss at iter 260 is 0.133757\n",
            "training loss at iter 280 is 0.117602\n",
            "training loss at iter 300 is 0.125154\n",
            "training loss at iter 320 is 0.134039\n",
            "training loss at iter 340 is 0.127082\n",
            "training loss at iter 360 is 0.149891\n",
            "training loss at iter 380 is 0.120612\n",
            "training loss at iter 400 is 0.116348\n",
            "training loss at iter 420 is 0.127711\n",
            "training loss at iter 440 is 0.133592\n",
            "training loss at iter 460 is 0.136229\n",
            "training loss at iter 480 is 0.122566\n",
            "training loss at iter 500 is 0.118632\n",
            "training loss at iter 520 is 0.130181\n",
            "training loss at iter 540 is 0.139573\n",
            "training loss at iter 560 is 0.136128\n",
            "training loss at iter 580 is 0.131841\n",
            "training loss at iter 600 is 0.118642\n",
            "training loss at iter 620 is 0.130727\n",
            "training loss at iter 640 is 0.120486\n",
            "training loss at iter 660 is 0.126460\n",
            "training loss at iter 680 is 0.120761\n",
            "training loss at iter 700 is 0.118117\n",
            "training loss at iter 720 is 0.136005\n",
            "training loss at iter 740 is 0.119037\n",
            "training loss at iter 760 is 0.125501\n",
            "training loss at iter 780 is 0.125110\n",
            "training loss at iter 800 is 0.130187\n",
            "training loss at iter 820 is 0.130404\n",
            "training loss at iter 840 is 0.132635\n",
            "training loss at iter 860 is 0.128278\n",
            "training loss at iter 880 is 0.124212\n",
            "training loss at iter 900 is 0.123648\n",
            "training loss at iter 920 is 0.130523\n",
            "training loss at iter 940 is 0.124835\n",
            "training loss at iter 960 is 0.120705\n",
            "training loss at iter 980 is 0.124734\n",
            "training loss at iter 1000 is 0.127626\n",
            "0.4257000000000003\n",
            "testing NMI, ARI, ACC at epoch 43 is 0.959209, 0.945874, 0.976562.\n",
            "0.5242999999999997 0.49757000000000007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.125621\n",
            "training loss at iter 40 is 0.124291\n",
            "training loss at iter 60 is 0.123428\n",
            "training loss at iter 80 is 0.132045\n",
            "training loss at iter 100 is 0.126693\n",
            "training loss at iter 120 is 0.128286\n",
            "training loss at iter 140 is 0.123060\n",
            "training loss at iter 160 is 0.124380\n",
            "training loss at iter 180 is 0.130914\n",
            "training loss at iter 200 is 0.116088\n",
            "training loss at iter 220 is 0.122320\n",
            "training loss at iter 240 is 0.119317\n",
            "training loss at iter 260 is 0.115050\n",
            "training loss at iter 280 is 0.127510\n",
            "training loss at iter 300 is 0.120978\n",
            "training loss at iter 320 is 0.119892\n",
            "training loss at iter 340 is 0.128919\n",
            "training loss at iter 360 is 0.125339\n",
            "training loss at iter 380 is 0.126920\n",
            "training loss at iter 400 is 0.125620\n",
            "training loss at iter 420 is 0.123937\n",
            "training loss at iter 440 is 0.120792\n",
            "training loss at iter 460 is 0.127562\n",
            "training loss at iter 480 is 0.134844\n",
            "training loss at iter 500 is 0.124959\n",
            "training loss at iter 520 is 0.120238\n",
            "training loss at iter 540 is 0.119249\n",
            "training loss at iter 560 is 0.125461\n",
            "training loss at iter 580 is 0.126822\n",
            "training loss at iter 600 is 0.124231\n",
            "training loss at iter 620 is 0.126289\n",
            "training loss at iter 640 is 0.153394\n",
            "training loss at iter 660 is 0.127412\n",
            "training loss at iter 680 is 0.124573\n",
            "training loss at iter 700 is 0.117676\n",
            "training loss at iter 720 is 0.131840\n",
            "training loss at iter 740 is 0.125279\n",
            "training loss at iter 760 is 0.131690\n",
            "training loss at iter 780 is 0.140671\n",
            "training loss at iter 800 is 0.123687\n",
            "training loss at iter 820 is 0.120188\n",
            "training loss at iter 840 is 0.124008\n",
            "training loss at iter 860 is 0.125392\n",
            "training loss at iter 880 is 0.120814\n",
            "training loss at iter 900 is 0.116813\n",
            "training loss at iter 920 is 0.127921\n",
            "training loss at iter 940 is 0.135375\n",
            "training loss at iter 960 is 0.124028\n",
            "training loss at iter 980 is 0.137005\n",
            "training loss at iter 1000 is 0.139329\n",
            "0.4356000000000003\n",
            "testing NMI, ARI, ACC at epoch 44 is 0.948471, 0.936864, 0.972656.\n",
            "0.5143999999999996 0.49856000000000006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.133822\n",
            "training loss at iter 40 is 0.119971\n",
            "training loss at iter 60 is 0.124720\n",
            "training loss at iter 80 is 0.133050\n",
            "training loss at iter 100 is 0.128434\n",
            "training loss at iter 120 is 0.132091\n",
            "training loss at iter 140 is 0.124966\n",
            "training loss at iter 160 is 0.126877\n",
            "training loss at iter 180 is 0.125675\n",
            "training loss at iter 200 is 0.132072\n",
            "training loss at iter 220 is 0.126639\n",
            "training loss at iter 240 is 0.124895\n",
            "training loss at iter 260 is 0.128481\n",
            "training loss at iter 280 is 0.137192\n",
            "training loss at iter 300 is 0.121146\n",
            "training loss at iter 320 is 0.122788\n",
            "training loss at iter 340 is 0.126582\n",
            "training loss at iter 360 is 0.136681\n",
            "training loss at iter 380 is 0.130438\n",
            "training loss at iter 400 is 0.123522\n",
            "training loss at iter 420 is 0.134455\n",
            "training loss at iter 440 is 0.132926\n",
            "training loss at iter 460 is 0.121524\n",
            "training loss at iter 480 is 0.135561\n",
            "training loss at iter 500 is 0.127164\n",
            "training loss at iter 520 is 0.127813\n",
            "training loss at iter 540 is 0.129767\n",
            "training loss at iter 560 is 0.149038\n",
            "training loss at iter 580 is 0.124804\n",
            "training loss at iter 600 is 0.125349\n",
            "training loss at iter 620 is 0.126243\n",
            "training loss at iter 640 is 0.123683\n",
            "training loss at iter 660 is 0.128029\n",
            "training loss at iter 680 is 0.136846\n",
            "training loss at iter 700 is 0.132662\n",
            "training loss at iter 720 is 0.124908\n",
            "training loss at iter 740 is 0.124739\n",
            "training loss at iter 760 is 0.130650\n",
            "training loss at iter 780 is 0.120982\n",
            "training loss at iter 800 is 0.133975\n",
            "training loss at iter 820 is 0.132944\n",
            "training loss at iter 840 is 0.121645\n",
            "training loss at iter 860 is 0.117612\n",
            "training loss at iter 880 is 0.132158\n",
            "training loss at iter 900 is 0.138130\n",
            "training loss at iter 920 is 0.124513\n",
            "training loss at iter 940 is 0.140112\n",
            "training loss at iter 960 is 0.132879\n",
            "training loss at iter 980 is 0.126762\n",
            "training loss at iter 1000 is 0.122919\n",
            "0.44550000000000034\n",
            "testing NMI, ARI, ACC at epoch 45 is 0.950761, 0.936176, 0.968750.\n",
            "Model saved in file: DAC_models/DAC_ep_45.ckpt\n",
            "0.5044999999999996 0.49955000000000005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.122273\n",
            "training loss at iter 40 is 0.116654\n",
            "training loss at iter 60 is 0.131121\n",
            "training loss at iter 80 is 0.122498\n",
            "training loss at iter 100 is 0.124764\n",
            "training loss at iter 120 is 0.131157\n",
            "training loss at iter 140 is 0.128475\n",
            "training loss at iter 160 is 0.125961\n",
            "training loss at iter 180 is 0.126790\n",
            "training loss at iter 200 is 0.122757\n",
            "training loss at iter 220 is 0.138188\n",
            "training loss at iter 240 is 0.135761\n",
            "training loss at iter 260 is 0.119640\n",
            "training loss at iter 280 is 0.135086\n",
            "training loss at iter 300 is 0.128450\n",
            "training loss at iter 320 is 0.129770\n",
            "training loss at iter 340 is 0.116304\n",
            "training loss at iter 360 is 0.133214\n",
            "training loss at iter 380 is 0.134833\n",
            "training loss at iter 400 is 0.117580\n",
            "training loss at iter 420 is 0.126938\n",
            "training loss at iter 440 is 0.125587\n",
            "training loss at iter 460 is 0.128665\n",
            "training loss at iter 480 is 0.126112\n",
            "training loss at iter 500 is 0.125144\n",
            "training loss at iter 520 is 0.118971\n",
            "training loss at iter 540 is 0.124125\n",
            "training loss at iter 560 is 0.120775\n",
            "training loss at iter 580 is 0.126025\n",
            "training loss at iter 600 is 0.125086\n",
            "training loss at iter 620 is 0.116834\n",
            "training loss at iter 640 is 0.142249\n",
            "training loss at iter 660 is 0.120023\n",
            "training loss at iter 680 is 0.131335\n",
            "training loss at iter 700 is 0.123457\n",
            "training loss at iter 720 is 0.137689\n",
            "training loss at iter 740 is 0.133176\n",
            "training loss at iter 760 is 0.131384\n",
            "training loss at iter 780 is 0.134420\n",
            "training loss at iter 800 is 0.126271\n",
            "training loss at iter 820 is 0.136627\n",
            "training loss at iter 840 is 0.129639\n",
            "training loss at iter 860 is 0.125234\n",
            "training loss at iter 880 is 0.125588\n",
            "training loss at iter 900 is 0.137619\n",
            "training loss at iter 920 is 0.120506\n",
            "training loss at iter 940 is 0.119852\n",
            "training loss at iter 960 is 0.126254\n",
            "training loss at iter 980 is 0.137514\n",
            "training loss at iter 1000 is 0.121031\n",
            "0.45540000000000036\n",
            "testing NMI, ARI, ACC at epoch 46 is 0.943675, 0.934256, 0.970703.\n",
            "0.4945999999999996 0.5005400000000001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training loss at iter 20 is 0.140386\n",
            "training loss at iter 40 is 0.123780\n",
            "training loss at iter 60 is 0.122276\n",
            "training loss at iter 80 is 0.120907\n",
            "training loss at iter 100 is 0.133124\n",
            "training loss at iter 120 is 0.121180\n",
            "training loss at iter 140 is 0.128807\n",
            "training loss at iter 160 is 0.124775\n",
            "training loss at iter 180 is 0.123807\n",
            "training loss at iter 200 is 0.120280\n",
            "training loss at iter 220 is 0.125723\n",
            "training loss at iter 240 is 0.117817\n",
            "training loss at iter 260 is 0.116450\n",
            "training loss at iter 280 is 0.118429\n",
            "training loss at iter 300 is 0.127243\n",
            "training loss at iter 320 is 0.130316\n",
            "training loss at iter 340 is 0.141857\n",
            "training loss at iter 360 is 0.126606\n",
            "training loss at iter 380 is 0.124468\n",
            "training loss at iter 400 is 0.126355\n",
            "training loss at iter 420 is 0.125211\n",
            "training loss at iter 440 is 0.120545\n",
            "training loss at iter 460 is 0.119184\n",
            "training loss at iter 480 is 0.132607\n",
            "training loss at iter 500 is 0.127184\n",
            "training loss at iter 520 is 0.128455\n",
            "training loss at iter 540 is 0.136534\n",
            "training loss at iter 560 is 0.122874\n",
            "training loss at iter 580 is 0.134776\n",
            "training loss at iter 600 is 0.131302\n",
            "training loss at iter 620 is 0.123085\n",
            "training loss at iter 640 is 0.124997\n",
            "training loss at iter 660 is 0.118928\n",
            "training loss at iter 680 is 0.125978\n",
            "training loss at iter 700 is 0.130665\n",
            "training loss at iter 720 is 0.118071\n",
            "training loss at iter 740 is 0.125381\n",
            "training loss at iter 760 is 0.130881\n",
            "training loss at iter 780 is 0.138413\n",
            "training loss at iter 800 is 0.119543\n",
            "training loss at iter 820 is 0.119261\n",
            "training loss at iter 840 is 0.130648\n",
            "training loss at iter 860 is 0.117782\n",
            "training loss at iter 880 is 0.119228\n",
            "training loss at iter 900 is 0.121547\n",
            "training loss at iter 920 is 0.130890\n",
            "training loss at iter 940 is 0.120858\n",
            "training loss at iter 960 is 0.125746\n",
            "training loss at iter 980 is 0.121480\n",
            "training loss at iter 1000 is 0.121346\n",
            "0.4653000000000004\n",
            "testing NMI, ARI, ACC at epoch 47 is 0.944304, 0.931997, 0.968750.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/linear_assignment_.py:128: FutureWarning: The linear_assignment function is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpV-LuqPzHdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}